"""
æœç´¢ä»£ç†ç±»
æ”¯æŒåŒæ¨¡å¼APIï¼šGoogle GenAI SDK å’Œ OpenAIå…¼å®¹HTTP API
"""

import time
import traceback
from typing import List, Dict, Optional, Any
from datetime import datetime

from .api_factory import APIClientFactory, ClientManager
from .api_client import BaseAPIClient, APIResponse
from .api_config import APIConfig, APIMode
from utils.helpers import (
    extract_json_from_text, 
    format_citations, 
    extract_urls,
    clean_text
)
from utils.debug_logger import get_debug_logger


class SearchAgent:
    """æ™ºèƒ½æœç´¢ä»£ç† - æ”¯æŒåŒæ¨¡å¼API"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash", preferred_mode: Optional[APIMode] = None):
        self.api_key = api_key
        self.model_name = model_name
        self.preferred_mode = preferred_mode
        self.search_history = []
        self.debug_logger = get_debug_logger()
        
        # ä½¿ç”¨å·¥å‚åˆ›å»ºå®¢æˆ·ç«¯
        self.client = APIClientFactory.create_search_client(
            api_key=api_key,
            model_name=model_name,
            preferred_mode=preferred_mode
        )
        
        # æ‰“å°å®¢æˆ·ç«¯ä¿¡æ¯
        client_info = APIClientFactory.get_client_info(model_name)
        print(f"ğŸ” æœç´¢ä»£ç†åˆå§‹åŒ–:")
        print(f"  æ¨¡å‹: {model_name}")
        print(f"  æ¨¡å¼: {client_info.get('mode', 'unknown')}")
        print(f"  æ”¯æŒæœç´¢: {client_info.get('supports_search', False)}")
        print(f"  æ”¯æŒå·¥å…·: {client_info.get('supports_tools', False)}")
    
    def _is_available(self) -> bool:
        """æ£€æŸ¥æœç´¢ä»£ç†æ˜¯å¦å¯ç”¨"""
        return self.client is not None
    
    async def search_with_grounding(self, query: str, use_search: bool = True) -> Dict[str, Any]:
        """
        ä½¿ç”¨é…ç½®çš„APIæ¨¡å¼è¿›è¡Œæœç´¢
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            use_search: æ˜¯å¦ä½¿ç”¨æœç´¢å·¥å…·
            
        Returns:
            åŒ…å«æœç´¢ç»“æœå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        if not self._is_available():
            raise Exception("æœç´¢ä»£ç†ä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥å®¢æˆ·ç«¯åˆå§‹åŒ–")
        
        try:
            search_start_time = datetime.now()
            
            # å‡†å¤‡å·¥å…·é…ç½®
            tools = []
            if use_search and self.client.supports_search():
                tools.append({"type": "web_search"})
            elif use_search and not self.client.supports_search():
                # å¦‚æœå®¢æˆ·ç«¯ä¸æ”¯æŒæœç´¢ä½†éœ€è¦æœç´¢ï¼Œè®°å½•è­¦å‘Š
                print(f"âš ï¸ æ¨¡å‹ {self.model_name} ä¸æ”¯æŒåŸç”Ÿæœç´¢ï¼Œå°†ä½¿ç”¨æ™®é€šå¯¹è¯æ¨¡å¼")
            
            # ä½¿ç”¨ç»Ÿä¸€çš„å®¢æˆ·ç«¯æ¥å£
            response = await self.client.generate_content(
                prompt=query,
                temperature=0.1,
                max_tokens=8192,
                tools=tools if tools else None
            )
            
            search_duration = (datetime.now() - search_start_time).total_seconds()
            
            # è½¬æ¢å“åº”æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§
            result = self._convert_to_legacy_format(response, query, search_duration)
            
            # Debug: è®°å½•æœç´¢ç»“æœ
            self.debug_logger.log_search_result(query, result, "dual_mode")
            
            # è®°å½•æœç´¢å†å²
            self.search_history.append({
                "query": query,
                "timestamp": datetime.now(),
                "duration": search_duration,
                "has_grounding": result.get("has_grounding", False),
                "api_mode": self.client.__class__.__name__
            })
            
            return result
            
        except Exception as e:
            error_result = {
                "success": False,
                "error": str(e),
                "query": query,
                "content": "",
                "citations": [],
                "urls": [],
                "has_grounding": False,
                "search_queries": [],
                "duration": 0
            }
            
            # Debug: è®°å½•é”™è¯¯
            self.debug_logger.log_error(
                error_type="SearchError",
                error_message=str(e),
                context={"query": query, "model": self.model_name, "client_type": self.client.__class__.__name__},
                stacktrace=traceback.format_exc()
            )
            
            return error_result
    
    def _convert_to_legacy_format(self, response: APIResponse, original_query: str, duration: float) -> Dict[str, Any]:
        """å°†æ–°çš„APIå“åº”è½¬æ¢ä¸ºæ—§çš„æ ¼å¼ä»¥ä¿æŒå…¼å®¹æ€§"""
        return {
            "success": response.success,
            "query": original_query,
            "content": response.text,
            "citations": response.citations,
            "urls": response.urls,
            "has_grounding": response.has_grounding,
            "search_queries": response.search_queries,
            "grounding_chunks": len(response.citations),
            "duration": duration,
            "error": response.error if not response.success else None
        }

    def _parse_search_response(self, response, original_query: str, duration: float) -> Dict[str, Any]:
        """ä¿ç•™æ—§æ–¹æ³•ä»¥ä¿æŒå‘åå…¼å®¹æ€§"""
        """è§£ææœç´¢å“åº”"""
        try:
            content = response.text if response and hasattr(response, 'text') else ""
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ grounding metadata
            has_grounding = False
            citations = []
            search_queries = []
            grounding_chunks = []
            
            if hasattr(response, 'candidates') and response.candidates:
                candidate = response.candidates[0]
                
                # æ£€æŸ¥ grounding metadata
                if hasattr(candidate, 'grounding_metadata') and candidate.grounding_metadata:
                    has_grounding = True
                    grounding_metadata = candidate.grounding_metadata
                    
                    # æå–æœç´¢æŸ¥è¯¢
                    if hasattr(grounding_metadata, 'web_search_queries'):
                        search_queries = list(grounding_metadata.web_search_queries)
                    
                    # æå– grounding chunks
                    if hasattr(grounding_metadata, 'grounding_chunks'):
                        grounding_chunks = grounding_metadata.grounding_chunks
                    
                    # æå– grounding supportsï¼ˆå¼•ç”¨ä¿¡æ¯ï¼‰
                    if hasattr(grounding_metadata, 'grounding_supports'):
                        citations = self._extract_citations(
                            grounding_metadata.grounding_supports,
                            grounding_chunks
                        )
            
            # æå–URL
            urls = extract_urls(content)
            
            # å¦‚æœæœ‰å¼•ç”¨ï¼Œæ ¼å¼åŒ–å†…å®¹
            if citations:
                content = format_citations(content, citations)
            
            return {
                "success": True,
                "query": original_query,
                "content": clean_text(content),
                "citations": citations,
                "urls": urls,
                "has_grounding": has_grounding,
                "search_queries": search_queries,
                "grounding_chunks": len(grounding_chunks),
                "duration": duration
            }
        except Exception as e:
            return {
                "success": False,
                "error": f"è§£æå“åº”å¤±è´¥: {str(e)}",
                "query": original_query,
                "content": "",
                "citations": [],
                "urls": [],
                "has_grounding": False,
                "search_queries": [],
                "duration": duration
            }
    
    def _extract_citations(self, grounding_supports, grounding_chunks) -> List[Dict]:
        """æå–å¼•ç”¨ä¿¡æ¯"""
        citations = []
        
        for support in grounding_supports:
            if hasattr(support, 'segment') and hasattr(support, 'grounding_chunk_indices'):
                start_index = getattr(support.segment, 'start_index', 0) 
                end_index = getattr(support.segment, 'end_index', 0)
                
                if end_index is None:
                    continue  # è·³è¿‡æ²¡æœ‰end_indexçš„é¡¹
                
                # è·å–å¯¹åº”çš„grounding chunks  
                chunk_citations = []
                for chunk_idx in support.grounding_chunk_indices:
                    if chunk_idx < len(grounding_chunks):
                        chunk = grounding_chunks[chunk_idx]
                        if hasattr(chunk, 'web') and chunk.web:
                            title = getattr(chunk.web, 'title', '') or 'Unknown Source'
                            uri = getattr(chunk.web, 'uri', '#')
                            
                            # æ¸…ç†æ ‡é¢˜ï¼ˆç§»é™¤æ–‡ä»¶æ‰©å±•åç­‰ï¼‰
                            if title and isinstance(title, str) and '.' in title:
                                title = title.split('.')[0]
                            
                            # æå–åŸŸå
                            domain = 'Unknown Domain'
                            if uri and '//' in uri:
                                try:
                                    domain = uri.split('//')[1].split('/')[0]
                                except:
                                    domain = 'Unknown Domain'
                            
                            chunk_citations.append({
                                "title": title,
                                "url": uri,
                                "description": f"æ¥æº: {domain}"
                            })
                
                if chunk_citations:
                    # ä¸ºäº†å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªchunkåˆ›å»ºå•ç‹¬çš„citation
                    for chunk_citation in chunk_citations:
                        citations.append({
                            "title": chunk_citation["title"],
                            "url": chunk_citation["url"], 
                            "description": chunk_citation["description"],
                            "start_index": start_index,
                            "end_index": end_index
                        })
        
        return citations
    
    async def generate_search_queries(self, user_query: str, num_queries: int = 3) -> List[str]:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢"""
        if not self._is_available():
            return [user_query]  # é™çº§åˆ°åŸå§‹æŸ¥è¯¢
        
        try:
            prompt = f"""
            åŸºäºä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œç”Ÿæˆ{num_queries}ä¸ªä¸åŒè§’åº¦çš„æœç´¢æŸ¥è¯¢ï¼Œå¸®åŠ©å…¨é¢ç ”ç©¶è¿™ä¸ªè¯é¢˜ã€‚
            
            ç”¨æˆ·æŸ¥è¯¢: {user_query}
            
            è¯·è¿”å›JSONæ ¼å¼çš„æŸ¥è¯¢åˆ—è¡¨ï¼Œä¾‹å¦‚ï¼š
            {{"queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3"]}}
            """
            
            # ä½¿ç”¨ç»Ÿä¸€çš„å®¢æˆ·ç«¯æ¥å£
            response = await self.client.generate_content(
                prompt=prompt,
                temperature=0.3,
                max_tokens=1024
            )
            
            if response.success and response.text:
                result = extract_json_from_text(response.text)
                if result and "queries" in result:
                    return result["queries"][:num_queries]
            
            return [user_query]  # é™çº§
            
        except Exception:
            return [user_query]  # é™çº§
    
    async def batch_search(self, queries: List[str]) -> List[Dict[str, Any]]:
        """æ‰¹é‡æœç´¢"""
        results = []
        
        for query in queries:
            result = await self.search_with_grounding(query)
            results.append(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
            time.sleep(1)
        
        return results
    
    def get_search_statistics(self) -> Dict[str, Any]:
        """è·å–æœç´¢ç»Ÿè®¡ä¿¡æ¯"""
        if not self.search_history:
            return {"total_searches": 0}
        
        total_searches = len(self.search_history)
        successful_searches = len([h for h in self.search_history if h.get("has_grounding", False)])
        avg_duration = sum(h.get("duration", 0) for h in self.search_history) / total_searches
        
        return {
            "total_searches": total_searches,
            "successful_searches": successful_searches,
            "success_rate": successful_searches / total_searches if total_searches > 0 else 0,
            "average_duration": avg_duration,
            "recent_queries": [h["query"] for h in self.search_history[-5:]]
        }
    
    def clear_history(self):
        """æ¸…é™¤æœç´¢å†å²"""
        self.search_history = [] 