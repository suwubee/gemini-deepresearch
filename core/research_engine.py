"""
ç ”ç©¶å¼•æ“æ ¸å¿ƒ
æ•´åˆå·¥ä½œæµæ„å»ºã€æœç´¢ä»£ç†ã€çŠ¶æ€ç®¡ç†ï¼Œå®ç°å®Œæ•´çš„æ·±åº¦ç ”ç©¶åŠŸèƒ½
"""

import asyncio
import json
import time
import traceback
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass
from datetime import datetime

# å¯¼å…¥æ ¸å¿ƒæ¨¡å—
from .search_agent import SearchAgent
from .workflow_builder import DynamicWorkflowBuilder, DynamicWorkflow, WorkflowStep
from .state_manager import StateManager
from .debug_logger import get_debug_logger
from ..utils.models import get_model_config, set_user_model


class ResearchEngine:
    """æ·±åº¦ç ”ç©¶å¼•æ“æ ¸å¿ƒ - æ”¯æŒåŒæ¨¡å¼API"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash"):
        """åˆå§‹åŒ–ç ”ç©¶å¼•æ“"""
        self.api_key = api_key
        self.model_name = model_name
        
        # è®¾ç½®ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼Œä½†æœç´¢åŠŸèƒ½å°†å›ºå®šä½¿ç”¨gemini-2.0-flash
        set_user_model(model_name)
        self.model_config = get_model_config()
        
        print(f"ğŸ¤– ç ”ç©¶å¼•æ“åˆå§‹åŒ–:")
        print(f"  ç”¨æˆ·é€‰æ‹©æ¨¡å‹: {model_name}")
        print(f"  æœç´¢æ¨¡å‹: {self.model_config.search_model}")
        print(f"  ä»»åŠ¡åˆ†ææ¨¡å‹: {self.model_config.task_analysis_model}")
        print(f"  åæ€æ¨¡å‹: {self.model_config.reflection_model}")
        print(f"  ç­”æ¡ˆç”Ÿæˆæ¨¡å‹: {self.model_config.answer_model}")
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼Œä½¿ç”¨åŸå§‹æ–¹æ³•
        self.workflow_builder = DynamicWorkflowBuilder(api_key, self.model_config.task_analysis_model)
        self.search_agent = SearchAgent(api_key, self.model_config.search_model)
        self.state_manager = StateManager()
        self.debug_logger = get_debug_logger()
        
        # è¿›åº¦å›è°ƒå‡½æ•°
        self.progress_callback: Optional[Callable] = None
        self.step_callback: Optional[Callable] = None
        self.error_callback: Optional[Callable] = None
        
        # åœæ­¢æ§åˆ¶æ ‡è®°
        self._stop_research = False
    
    def set_callbacks(self, progress_callback=None, step_callback=None, error_callback=None):
        """è®¾ç½®å›è°ƒå‡½æ•°"""
        if progress_callback:
            self.progress_callback = progress_callback
        if step_callback:
            self.step_callback = step_callback
        if error_callback:
            self.error_callback = error_callback
    
    def set_progress_callback(self, callback: Callable):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
    
    def set_step_callback(self, callback: Callable):
        """è®¾ç½®æ­¥éª¤å›è°ƒå‡½æ•°"""
        self.step_callback = callback
    
    def set_error_callback(self, callback: Callable):
        """è®¾ç½®é”™è¯¯å›è°ƒå‡½æ•°"""
        self.error_callback = callback
    
    def stop_research(self):
        """åœæ­¢å½“å‰ç ”ç©¶"""
        self._stop_research = True
        self._notify_step("ğŸ›‘ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨ç»ˆæ­¢ç ”ç©¶...")
    
    def reset_stop_flag(self):
        """é‡ç½®åœæ­¢æ ‡è®°"""
        self._stop_research = False
    
    async def research(self, user_query: str, 
                      max_search_rounds: int = 3,
                      effort_level: str = "medium",
                      num_search_queries: int = 3) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ·±åº¦ç ”ç©¶
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            max_search_rounds: æœ€å¤§æœç´¢è½®æ•°
            effort_level: åŠªåŠ›çº§åˆ« (low, medium, high)
            num_search_queries: åˆå§‹æœç´¢æŸ¥è¯¢æ•°é‡
            
        Returns:
            ç ”ç©¶ç»“æœå­—å…¸
        """
        try:
            # é‡ç½®åœæ­¢æ ‡è®°
            self._stop_research = False
            
            # Debug: è®°å½•ç ”ç©¶å¼€å§‹
            self.debug_logger.log_workflow_step(
                step_name="research_start",
                step_status="running",
                input_data={
                    "user_query": user_query,
                    "max_search_rounds": max_search_rounds,
                    "effort_level": effort_level,
                    "num_search_queries": num_search_queries
                }
            )
            
            # 1. å¼€å§‹æ–°ä»»åŠ¡
            task_id = self.state_manager.start_new_task(user_query)
            self._notify_progress("å¼€å§‹åˆ†æä»»åŠ¡...", 0)
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if self._stop_research:
                return {"success": False, "error": "ç ”ç©¶è¢«ç”¨æˆ·åœæ­¢"}
            
            # 2. åˆ†æä»»åŠ¡å¹¶æ„å»ºå·¥ä½œæµ
            workflow = await self._analyze_and_build_workflow(user_query, effort_level)
            
            # è®¾ç½®æœç´¢æŸ¥è¯¢æ•°é‡
            workflow.config["num_search_queries"] = num_search_queries
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if self._stop_research:
                return {"success": False, "error": "ç ”ç©¶è¢«ç”¨æˆ·åœæ­¢"}
            
            # 3. æ›¿æ¢å·¥ä½œæµæ­¥éª¤å‡½æ•°ä¸ºå®é™…å®ç°
            self._inject_research_functions(workflow)
            
            # 4. æ‰§è¡Œå·¥ä½œæµ
            self.state_manager.update_task_progress(status=TaskStatus.ANALYZING)
            result = await self._execute_workflow(workflow, user_query, max_search_rounds, num_search_queries)
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if self._stop_research:
                return {"success": False, "error": "ç ”ç©¶è¢«ç”¨æˆ·åœæ­¢"}
            
            # 5. å®Œæˆä»»åŠ¡
            self.state_manager.complete_task(result)
            self._notify_progress("ç ”ç©¶å®Œæˆï¼", 100)
            
            final_result = {
                "success": True,
                "task_id": task_id,
                "user_query": user_query,
                "final_answer": result.get("final_answer", ""),
                "search_results": self.state_manager.get_successful_search_results(),
                "citations": self.state_manager.get_all_citations(),
                "urls": self.state_manager.get_unique_urls(),
                "workflow_analysis": self.state_manager.workflow_analysis,
                "task_summary": self.state_manager.get_task_summary(),
                "statistics": self.state_manager.get_session_statistics()
            }
            
            # Debug: è®°å½•ç ”ç©¶å®Œæˆ
            self.debug_logger.log_research_result(
                user_query=user_query,
                final_result=final_result,
                metadata={
                    "task_id": task_id,
                    "effort_level": effort_level,
                    "max_search_rounds": max_search_rounds
                }
            )
            
            self.debug_logger.log_workflow_step(
                step_name="research_complete",
                step_status="completed",
                output_data=final_result
            )
            
            return final_result
            
        except Exception as e:
            # å¤„ç†é”™è¯¯
            error_msg = f"ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            self.state_manager.fail_task(error_msg)
            
            # Debug: è®°å½•é”™è¯¯
            self.debug_logger.log_error(
                error_type="ResearchError",
                error_message=error_msg,
                context={
                    "user_query": user_query,
                    "max_search_rounds": max_search_rounds,
                    "effort_level": effort_level
                },
                stacktrace=traceback.format_exc()
            )
            
            self.debug_logger.log_workflow_step(
                step_name="research_failed",
                step_status="failed",
                output_data={"error": error_msg}
            )
            
            if self.error_callback:
                self.error_callback(error_msg)
            
            return {
                "success": False,
                "error": error_msg,
                "task_summary": self.state_manager.get_task_summary()
            }
    
    async def _analyze_and_build_workflow(self, user_query: str, effort_level: str) -> DynamicWorkflow:
        """åˆ†æä»»åŠ¡å¹¶æ„å»ºå·¥ä½œæµ"""
        self._notify_step("æ­£åœ¨åˆ†æä»»åŠ¡ç±»å‹...")
        self._notify_progress("å¼€å§‹ä»»åŠ¡åˆ†æ", 10)
        
        try:
            workflow = await self.workflow_builder.analyze_task_and_build_workflow(user_query)
            
            # ä¿å­˜å·¥ä½œæµåˆ†æåˆ°çŠ¶æ€ç®¡ç†å™¨
            self.state_manager.set_workflow_analysis(workflow.config)
            self._notify_progress("ä»»åŠ¡åˆ†æå®Œæˆ", 20)
            
            # æ ¹æ®åŠªåŠ›çº§åˆ«è°ƒæ•´å‚æ•°
            self._adjust_workflow_by_effort(workflow, effort_level)
            
            task_type = workflow.config.get("task_type", "Q&A System")
            complexity = workflow.config.get("complexity", "Medium")
            estimated_steps = len(workflow.steps_config)  # ä½¿ç”¨æ­¥éª¤é…ç½®æ•°é‡
            
            print(f"ğŸ” å·¥ä½œæµè¯¦æƒ…: ç±»å‹={task_type}, å¤æ‚åº¦={complexity}, å®é™…æ­¥éª¤={estimated_steps}")
            print(f"ğŸ” æ­¥éª¤åˆ—è¡¨: {[step['name'] for step in workflow.steps_config]}")
            
            self._notify_step(f"ä»»åŠ¡ç±»å‹: {task_type} (å¤æ‚åº¦: {complexity})")
            self._notify_progress(f"å·¥ä½œæµæ„å»ºå®Œæˆï¼Œé¢„è®¡{estimated_steps}æ­¥", 30)
            
            return workflow
            
        except Exception as e:
            error_msg = f"å·¥ä½œæµæ„å»ºå¤±è´¥: {str(e)}"
            self._notify_step(error_msg)
            print(f"âŒ å·¥ä½œæµæ„å»ºå¼‚å¸¸: {e}")
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„é™çº§å·¥ä½œæµ
            return self._create_fallback_workflow()
    
    def _create_fallback_workflow(self) -> DynamicWorkflow:
        """åˆ›å»ºé™çº§å·¥ä½œæµ"""
        workflow_config = {
            "task_type": "é—®ç­”ç³»ç»Ÿ",
            "complexity": "ç®€å•",
            "requires_search": True,
            "requires_multiple_rounds": False,
            "estimated_steps": 2,
            "estimated_time": "1-2åˆ†é’Ÿ",
            "reasoning": "é™çº§å·¥ä½œæµï¼šç›´æ¥æœç´¢å’Œå›ç­”"
        }
        steps_config = [
            {"name": "simple_search", "description": "æ‰§è¡Œç®€å•çš„ç½‘ç»œæœç´¢"},
            {"name": "generate_final_answer", "description": "ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"}
        ]
        return DynamicWorkflow(workflow_config, steps_config)
    
    def _adjust_workflow_by_effort(self, workflow: DynamicWorkflow, effort_level: str):
        """æ ¹æ®åŠªåŠ›çº§åˆ«è°ƒæ•´å·¥ä½œæµå‚æ•°ï¼ˆå‚è€ƒåŸå§‹frontendè§„æ ¼ï¼‰"""
        # å‚è€ƒåŸå§‹frontend: 
        # Low: initial_search_query_count=1, max_research_loops=1
        # Medium: initial_search_query_count=3, max_research_loops=3  
        # High: initial_search_query_count=5, max_research_loops=10
        
        # å¼ºåˆ¶è¦†ç›–AIåˆ†æçš„å¤æ‚åº¦ï¼Œä»¥ç”¨æˆ·é€‰æ‹©çš„effortä¸ºå‡†
        if effort_level == "low":
            workflow.config["complexity"] = "Low"
            workflow.config["estimated_steps"] = 4
            workflow.config["estimated_time"] = "1-5åˆ†é’Ÿ"
            workflow.config["max_search_rounds"] = 3  # ä½å¼ºåº¦ï¼šæœ€å¤š3è½®æœç´¢ï¼ˆåŒ…æ‹¬è¡¥å……ï¼‰
            workflow.config["default_search_rounds"] = 1  # é»˜è®¤1è½®ï¼Œä¸è¶³æ—¶è¡¥å……
            workflow.config["queries_per_round"] = 3  # æ¯è½®3ä¸ªæŸ¥è¯¢
            self.state_manager.update_settings(
                max_search_results=15,
                max_iterations=3,
                search_timeout=30
            )
        elif effort_level == "high":
            workflow.config["complexity"] = "High"  
            workflow.config["estimated_steps"] = 8
            workflow.config["estimated_time"] = "8-20åˆ†é’Ÿ"
            workflow.config["max_search_rounds"] = 5  # é«˜å¼ºåº¦ï¼š5è½®æœç´¢
            workflow.config["default_search_rounds"] = 5
            workflow.config["queries_per_round"] = 10  # æ¯è½®10ä¸ªæŸ¥è¯¢
            self.state_manager.update_settings(
                max_search_results=50,
                max_iterations=5,
                search_timeout=60
            )
        else:  # medium
            workflow.config["complexity"] = "Medium"
            workflow.config["estimated_steps"] = 6
            workflow.config["estimated_time"] = "4-10åˆ†é’Ÿ"
            workflow.config["max_search_rounds"] = 3  # ä¸­ç­‰å¼ºåº¦ï¼š3è½®æœç´¢
            workflow.config["default_search_rounds"] = 3
            workflow.config["queries_per_round"] = 5  # æ¯è½®5ä¸ªæŸ¥è¯¢
            self.state_manager.update_settings(
                max_search_results=25,
                max_iterations=3,
                search_timeout=45
            )
        
        print(f"ğŸ¯ ç”¨æˆ·effortçº§åˆ«: {effort_level} â†’ å¤æ‚åº¦: {workflow.config['complexity']}, æœ€å¤§æœç´¢è½®æ•°: {workflow.config['max_search_rounds']}")
    
    def _inject_research_functions(self, workflow: DynamicWorkflow):
        """å°†å®é™…çš„ç ”ç©¶å‡½æ•°æ³¨å…¥åˆ°å·¥ä½œæµæ­¥éª¤ä¸­"""
        
        function_mapping = {
            "generate_search_queries": self._generate_search_queries_step,
            "execute_search": self._execute_search_step,
            "analyze_search_results": self._analyze_search_results_step,
            "supplementary_search": self._supplementary_search_step,
            "generate_final_answer": self._generate_final_answer_step,
            "simple_search": self._simple_search_step,
        }

        # æ›¿æ¢ workflow.steps ä¸ºç»‘å®šäº†å‡½æ•°çš„å®Œæ•´ WorkflowStep å®ä¾‹
        injected_steps = []
        for step_config in workflow.steps_config:
            step_name = step_config["name"]
            if step_name in function_mapping:
                step_function = function_mapping[step_name]
                injected_steps.append(WorkflowStep(
                    name=step_name,
                    description=step_config["description"],
                    function=step_function
                ))
            else:
                raise ValueError(f"æœªæ‰¾åˆ°å·¥ä½œæµæ­¥éª¤ '{step_name}' å¯¹åº”çš„å®ç°å‡½æ•°")
        
        workflow.steps = injected_steps
    
    async def _execute_workflow(self, workflow: DynamicWorkflow, 
                               user_query: str, max_search_rounds: int, num_search_queries: int = 3) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµï¼ŒåŒ…å«å¯èƒ½çš„å¤šè½®ç ”ç©¶"""
        
        # ä½¿ç”¨å·¥ä½œæµé…ç½®ä¸­çš„max_search_roundsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        effective_max_rounds = workflow.config.get("max_search_rounds", max_search_rounds)
        
        context = {
            "user_query": user_query,
            "max_search_rounds": effective_max_rounds,
            "num_search_queries": num_search_queries,
            "queries_per_round": workflow.config.get("queries_per_round", num_search_queries),
            "effort_level": workflow.config.get("complexity", "Medium").lower()
        }
        
        print(f"ğŸ”„ æ‰§è¡Œå·¥ä½œæµï¼Œæœ€å¤§æœç´¢è½®æ•°: {effective_max_rounds}")
        
        # æ‰§è¡Œåˆå§‹æ­¥éª¤ï¼Œç›´åˆ°éœ€è¦å¾ªç¯çš„"è¡¥å……æœç´¢"æˆ–"æœ€ç»ˆç­”æ¡ˆ"
        for step in workflow.steps:
            if step.name == "supplementary_search":
                self._notify_step(f"âœ… åˆå§‹æœç´¢é˜¶æ®µå®Œæˆï¼Œå‡†å¤‡è¿›å…¥è¡¥å……æœç´¢å¾ªç¯")
                break # ç»“æŸåˆå§‹æ­¥éª¤çš„æ‰§è¡Œï¼Œä½†ä¸è·³è¿‡simple_search
            elif step.name == "generate_final_answer":
                # å¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œåœ¨å¾ªç¯å¤–å•ç‹¬å¤„ç†
                self._notify_step(f"âœ… æ‰€æœ‰æœç´¢æ­¥éª¤å®Œæˆï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
                break
            
            self._notify_step(f"ğŸ”„ æ‰§è¡Œæ­¥éª¤: {step.name}")
            
            # Debug: è®°å½•æ­¥éª¤å¼€å§‹
            step_start_time = time.time()
            self.debug_logger.log_workflow_step(
                step_name=step.name,
                step_status="running",
                input_data=context
            )
            
            try:
                result = await self._execute_step_with_context(step, context)
                context.update(result)
                
                # Debug: è®°å½•æ­¥éª¤å®Œæˆ
                step_duration = time.time() - step_start_time
                self.debug_logger.log_workflow_step(
                    step_name=step.name,
                    step_status="completed",
                    input_data=context,
                    output_data=result,
                    duration=step_duration
                )
                
                self._notify_step(f"âœ… æ­¥éª¤ {step.name} å®Œæˆ [{step_duration:.2f}s]")
                
            except Exception as e:
                # Debug: è®°å½•æ­¥éª¤å¤±è´¥
                step_duration = time.time() - step_start_time
                self.debug_logger.log_workflow_step(
                    step_name=step.name,
                    step_status="failed",
                    input_data=context,
                    duration=step_duration,
                    error_message=str(e)
                )
                raise
            
        # å¦‚æœå®šä¹‰äº†è¡¥å……æœç´¢ï¼Œåˆ™è¿›å…¥å¾ªç¯ï¼ˆæ‰€æœ‰å¼ºåº¦éƒ½æ”¯æŒè¡¥å……æœç´¢ï¼‰
        supplementary_search_step = next((s for s in workflow.steps if s.name == "supplementary_search"), None)
        if supplementary_search_step:
            current_round = 1
            default_rounds = workflow.config.get("default_search_rounds", 1)
            
            # Debug: è®°å½•è¡¥å……æœç´¢å¾ªç¯å¼€å§‹
            self.debug_logger.log_execution_flow(
                flow_type="supplementary_search_loop",
                description=f"å¼€å§‹è¡¥å……æœç´¢å¾ªç¯ï¼Œé»˜è®¤è½®æ•°: {default_rounds}, æœ€å¤§è½®æ•°: {effective_max_rounds}",
                details={
                    "default_rounds": default_rounds,
                    "max_rounds": effective_max_rounds,
                    "effort_level": context.get("effort_level", "medium")
                }
            )
            
            while current_round < effective_max_rounds:
                # æ£€æŸ¥åœæ­¢ä¿¡å·
                if self._stop_research:
                    self._notify_step("ğŸ›‘ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œç»ˆæ­¢è¡¥å……æœç´¢")
                    break
                
                # å¦‚æœå·²ç»è¾¾åˆ°é»˜è®¤è½®æ•°ï¼Œæ£€æŸ¥ä¿¡æ¯æ˜¯å¦å……è¶³
                if current_round >= default_rounds:
                    is_sufficient = context.get("is_sufficient", False)
                    
                    # Debug: è®°å½•ä¿¡æ¯å……è¶³æ€§æ£€æŸ¥
                    self.debug_logger.log_decision_point(
                        decision_type="information_sufficiency_check",
                        condition=f"current_round({current_round}) >= default_rounds({default_rounds})",
                        result=f"is_sufficient: {is_sufficient}",
                        context={
                            "current_round": current_round,
                            "default_rounds": default_rounds,
                            "is_sufficient": is_sufficient
                        }
                    )
                    
                    if is_sufficient:
                        self._notify_step("âœ… ä¿¡æ¯å·²å……è¶³ï¼Œè·³è¿‡åç»­è¡¥å……ç ”ç©¶")
                        break
                    else:
                        # å¯¹äºä½å¼ºåº¦ï¼Œåœ¨ç¬¬2è½®åæ›´ä¸¥æ ¼åœ°æ£€æŸ¥æ˜¯å¦åœæ­¢
                        effort_level = context.get("effort_level", "medium")
                        if effort_level == "low" and current_round >= 2:
                            total_content = sum(len(content) for content in self.state_manager.get_search_content_list())
                            
                            # Debug: è®°å½•ä½å¼ºåº¦ç‰¹æ®Šæ£€æŸ¥
                            self.debug_logger.log_decision_point(
                                decision_type="low_effort_content_check",
                                condition=f"effort_level=low AND current_round({current_round})>=2 AND total_content({total_content})>800",
                                result=f"stop_search: {total_content > 800}",
                                context={
                                    "effort_level": effort_level,
                                    "current_round": current_round,
                                    "total_content": total_content,
                                    "threshold": 800
                                }
                            )
                            
                            if total_content > 800:  # ä½å¼ºåº¦çš„æ›´å®½æ¾æ¡ä»¶
                                self._notify_step("âœ… ä½å¼ºåº¦æ¨¡å¼ï¼šä¿¡æ¯é‡å·²è¶³å¤Ÿï¼Œåœæ­¢è¡¥å……æœç´¢")
                                break
                        
                        # å¯¹äºæ‰€æœ‰å¼ºåº¦ï¼Œåœ¨ç¬¬3è½®åå¼ºåˆ¶æ£€æŸ¥
                        if current_round >= 3:
                            total_content = sum(len(content) for content in self.state_manager.get_search_content_list())
                            
                            # Debug: è®°å½•å¼ºåˆ¶æ£€æŸ¥
                            self.debug_logger.log_decision_point(
                                decision_type="force_content_check",
                                condition=f"current_round({current_round})>=3 AND total_content({total_content})>1200",
                                result=f"force_stop: {total_content > 1200}",
                                context={
                                    "effort_level": effort_level,
                                    "current_round": current_round,
                                    "total_content": total_content,
                                    "threshold": 1200
                                }
                            )
                            
                            if total_content > 1200:  # å¼ºåˆ¶åœæ­¢æ¡ä»¶
                                self._notify_step("âœ… ä¿¡æ¯é‡å……è¶³ï¼Œå¼ºåˆ¶åœæ­¢è¡¥å……æœç´¢")
                                break
                        self._notify_step(f"â„¹ï¸ å·²å®Œæˆé»˜è®¤{default_rounds}è½®æœç´¢ï¼Œä½†ä¿¡æ¯ä¸è¶³ï¼Œç»§ç»­è¡¥å……æœç´¢...")
                
                # è®¡ç®—å½“å‰è½®æ¬¡çš„è¿›åº¦
                base_progress = 60  # åˆå§‹æœç´¢å®Œæˆåçš„è¿›åº¦
                round_progress = base_progress + (current_round * 15)  # æ¯è½®å¢åŠ 15%
                
                self._notify_step(f"ğŸ”„ ç¬¬ {current_round+1}/{effective_max_rounds} è½®è¡¥å……ç ”ç©¶å¼€å§‹...")
                self._notify_progress(f"æ‰§è¡Œç¬¬ {current_round+1} è½®è¡¥å……æœç´¢", round_progress)
                
                # æ‰§è¡Œè¡¥å……æœç´¢ï¼Œä¼ é€’è½®æ¬¡ä¿¡æ¯
                context["current_round"] = current_round + 1
                context["total_rounds"] = effective_max_rounds
                context["default_rounds"] = default_rounds
                context["queries_per_round"] = workflow.config.get("queries_per_round", num_search_queries)
                result = await self._execute_step_with_context(supplementary_search_step, context)
                context.update(result)
                
                # å†æ¬¡åˆ†æç»“æœ
                analyze_step = next((s for s in workflow.steps if s.name == "analyze_search_results"), None)
                if analyze_step:
                    context["search_round"] = current_round  # ä¼ é€’æœç´¢è½®æ¬¡ç»™åˆ†ææ­¥éª¤
                    context["effort_level"] = context.get("effort_level", "medium")  # ç¡®ä¿ä¼ é€’effort_level
                    analysis_result = await self._execute_step_with_context(analyze_step, context)
                    context.update(analysis_result)

                current_round += 1
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§è½®æ•°ä½†ä¿¡æ¯ä»ä¸å……è¶³
            if current_round >= effective_max_rounds and not context.get("is_sufficient"):
                self._notify_step(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æœç´¢è½®æ•°({effective_max_rounds})ï¼Œåœæ­¢è¡¥å……æœç´¢")

        # å¾ªç¯ç»“æŸï¼Œæ‰§è¡Œæœ€ç»ˆçš„ç­”æ¡ˆç”Ÿæˆ
        final_answer_step = next((s for s in workflow.steps if s.name == "generate_final_answer"), None)
        if final_answer_step:
            final_result = await self._execute_step_with_context(final_answer_step, context)
            context.update(final_result)
        else:
            raise ValueError("å·¥ä½œæµä¸­æœªå®šä¹‰ 'generate_final_answer' æ­¥éª¤")
        
        return context
    
    async def _execute_step_with_context(self, step: WorkflowStep, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµæ­¥éª¤å¹¶è¿”å›ç»“æœ"""
        return await step.execute(context)
    
    async def _generate_search_queries_step(self, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢æ­¥éª¤"""
        user_query = kwargs.get("user_query", "")
        # ä¼˜å…ˆä½¿ç”¨workflowé…ç½®çš„æŸ¥è¯¢æ•°é‡
        workflow_queries = kwargs.get("queries_per_round")
        num_queries = workflow_queries or kwargs.get("num_search_queries", kwargs.get("num_queries", 3))
        
        self._notify_step(f"æ­£åœ¨ç”Ÿæˆ {num_queries} ä¸ªæœç´¢æŸ¥è¯¢...")
        
        # Debug: è®°å½•APIè¯·æ±‚
        request_id = f"gen_queries_{int(time.time() * 1000)}"
        self.debug_logger.log_api_request(
            request_type="generate_search_queries",
            model=self.model_config.search_model,
            prompt=f"ä¸ºç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆ{num_queries}ä¸ªæœç´¢æŸ¥è¯¢: {user_query}",
            request_id=request_id,
            context="ç”Ÿæˆæœç´¢æŸ¥è¯¢"
        )
        
        queries = await self.search_agent.generate_search_queries(user_query, num_queries)
        
        # Debug: è®°å½•APIå“åº”
        self.debug_logger.log_api_response(
            request_id=request_id,
            response_text=f"ç”Ÿæˆäº†{len(queries)}ä¸ªæŸ¥è¯¢: {queries}"
        )
        
        self.state_manager.add_search_queries(queries)
        
        return {"search_queries": queries}
    
    async def _execute_search_step(self, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢æ­¥éª¤"""
        search_queries = kwargs.get("search_queries", [kwargs.get("user_query", "")])
        
        self._notify_step("æ­£åœ¨æ‰§è¡Œç½‘ç»œæœç´¢...")
        
        search_results = []
        for i, query in enumerate(search_queries):
            self._notify_progress(f"æœç´¢æŸ¥è¯¢ {i+1}/{len(search_queries)}: {query[:30]}...", 
                                 40 + (i * 20 // len(search_queries)))
            
            # Debug: è®°å½•æœç´¢è¯·æ±‚
            search_request_id = f"search_{i}_{int(time.time() * 1000)}"
            self.debug_logger.log_api_request(
                request_type="grounding_search",
                model=self.model_config.search_model,
                prompt=f"æœç´¢æŸ¥è¯¢: {query}",
                request_id=search_request_id,
                context=f"æœç´¢æŸ¥è¯¢ {i+1}/{len(search_queries)}"
            )
            
            result = await self.search_agent.search_with_grounding(query)
            
            # Debug: è®°å½•æœç´¢ç»“æœ
            self.debug_logger.log_search_result(query, result, "grounding")
            
            # Debug: è®°å½•æœç´¢å“åº”
            if result.get("success"):
                response_summary = f"æˆåŠŸè·å–å†…å®¹ï¼Œé•¿åº¦: {len(result.get('content', ''))}, å¼•ç”¨æ•°: {len(result.get('citations', []))}"
            else:
                response_summary = f"æœç´¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
            
            self.debug_logger.log_api_response(
                request_id=search_request_id,
                response_text=response_summary,
                metadata={
                    "success": result.get("success", False),
                    "content_length": len(result.get("content", "")),
                    "citations_count": len(result.get("citations", [])),
                    "urls_count": len(result.get("urls", []))
                },
                error=None if result.get("success") else result.get("error", "æœç´¢å¤±è´¥")
            )
            
            if result.get("success"):
                # å°†ç»“æœæ·»åŠ åˆ°çŠ¶æ€ç®¡ç†å™¨ï¼ˆä¼šè½¬æ¢ä¸º SearchResult å¯¹è±¡ï¼‰
                self.state_manager.add_search_result(query, result)
                
                # ä¿å­˜åˆ°åˆ†æè¿‡ç¨‹ï¼ˆå‚è€ƒåŸå§‹backendç»“æ„ï¼‰
                web_research_content = f"Query: {query}\nContent: {result.get('content', '')}"
                if result.get('citations'):
                    citations_list = result.get('citations', []) or []
                    citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" 
                                               for cite in citations_list[:3]])
                    web_research_content += f"\nCitations:\n{citations_text}"
                self.state_manager.add_web_research_result(web_research_content)
                
                # åœ¨å·¥ä½œæµæ­¥éª¤é—´ä¼ é€’å­—å…¸æ ¼å¼
                search_results.append(result)
        
        return {"search_results": search_results}
    
    async def _analyze_search_results_step(self, **kwargs) -> Dict[str, Any]:
        """åˆ†ææœç´¢ç»“æœæ­¥éª¤ - å‚è€ƒåŸå§‹backendçš„reflectioné€»è¾‘"""
        user_query = kwargs.get("user_query", "")
        search_results = kwargs.get("search_results", [])
        search_round = kwargs.get("search_round", 0)
        current_round = kwargs.get("current_round", search_round + 1)
        total_rounds = kwargs.get("total_rounds", kwargs.get("max_search_rounds", 3))
        
        self._notify_step(f"ğŸ¤” åˆ†æç¬¬ {current_round} è½®æœç´¢ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­...")
        self._notify_progress(f"åˆ†æç¬¬ {current_round} è½®ç»“æœ", 70 + ((current_round - 1) * 10))
        
        # è·å–æ‰€æœ‰å·²æœç´¢çš„å†…å®¹ç”¨äºåæ€åˆ†æ
        all_research_results = self.state_manager.get_search_content_list()
        
        # ä½¿ç”¨AIè¿›è¡Œåæ€åˆ†æï¼ˆå‚è€ƒåŸå§‹backendçš„reflectionå‡½æ•°ï¼‰
        from utils.prompts import PromptTemplates
        
        reflection_prompt = f"""åˆ†æä»¥ä¸‹ç ”ç©¶å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æœç´¢ï¼š

ç ”ç©¶ä¸»é¢˜: {user_query}
å½“å‰æœç´¢è½®æ•°: {current_round}
æœ€å¤§æœç´¢è½®æ•°: {total_rounds}

å·²æ”¶é›†çš„ç ”ç©¶å†…å®¹:
{chr(10).join(['---' + chr(10) + content for content in all_research_results[-5:]])}

è¯·å›ç­”ï¼š
1. å½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜ï¼Ÿ(æ˜¯/å¦)
2. å¦‚æœä¸å¤Ÿï¼Œè¿˜éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
3. å»ºè®®çš„åç»­æœç´¢æŸ¥è¯¢ï¼ˆå¦‚æœéœ€è¦ï¼‰

è¯·ä»¥JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_sufficient": true/false,
    "knowledge_gap": "è¯´æ˜ä¿¡æ¯ç¼ºå£",
    "follow_up_queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]
}}"""

        try:
            if self.search_agent.client:
                reflection_model = self.model_config.get_model_for_task("reflection")
                max_tokens = self.model_config.get_token_limits("reflection")
                
                # Debug: è®°å½•åæ€åˆ†æAPIè¯·æ±‚
                reflection_request_id = f"reflection_{current_round}_{int(time.time() * 1000)}"
                self.debug_logger.log_api_request(
                    request_type="reflection_analysis",
                    model=reflection_model,
                    prompt=reflection_prompt,
                    request_id=reflection_request_id,
                    context=f"ç¬¬{current_round}è½®åæ€åˆ†æ"
                )
                
                response = self.search_agent.client.models.generate_content(
                    model=reflection_model,
                    contents=reflection_prompt,
                    config={
                        "temperature": 0.3,
                        "max_output_tokens": max_tokens
                    }
                )
                
                # Debug: è®°å½•åæ€åˆ†æAPIå“åº”
                self.debug_logger.log_api_response(
                    request_id=reflection_request_id,
                    response_text=response.text
                )
                
                # è§£æAIåæ€ç»“æœ
                import json
                try:
                    reflection_result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))
                except:
                    # é™çº§å¤„ç†
                    reflection_result = {
                        "is_sufficient": len(all_research_results) >= 2,
                        "knowledge_gap": "éœ€è¦æ›´å¤šè¯¦ç»†ä¿¡æ¯",
                        "follow_up_queries": [f"{user_query} è¯¦ç»†åˆ†æ", f"{user_query} æœ€æ–°å‘å±•"]
                    }
            else:
                # é™çº§å¤„ç†ï¼šç®€å•çš„é•¿åº¦åˆ¤æ–­
                total_content = sum(len(content) for content in all_research_results)
                effort_level = kwargs.get("effort_level", "medium")
                
                # æ ¹æ®å¼ºåº¦å’Œè½®æ¬¡è°ƒæ•´é˜ˆå€¼
                if effort_level == "low":
                    content_threshold = 800 if current_round >= 2 else 1200
                elif effort_level == "medium":
                    content_threshold = 1500 if current_round >= 2 else 2000
                else:  # high
                    content_threshold = 2000 if current_round >= 3 else 2500
                
                reflection_result = {
                    "is_sufficient": total_content > content_threshold or current_round >= total_rounds,
                    "knowledge_gap": "ä¿¡æ¯å……è¶³" if total_content > content_threshold else "éœ€è¦æ›´å¤šè¯¦ç»†ä¿¡æ¯",
                    "follow_up_queries": [] if total_content > content_threshold else [f"{user_query} è¯¦ç»†åˆ†æ"]
                }
        
        except Exception as e:
            self._notify_step(f"åæ€åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•é€»è¾‘: {str(e)}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯APIé…é¢è€—å°½é”™è¯¯
            error_str = str(e).lower()
            is_quota_exhausted = any(keyword in error_str for keyword in [
                "quota", "resource_exhausted", "rate limit", "429"
            ])
            
            if is_quota_exhausted:
                # å¦‚æœæ˜¯é…é¢è€—å°½ï¼Œå¼ºåˆ¶æ ‡è®°ä¸ºä¿¡æ¯å……è¶³ï¼Œåœæ­¢å¾ªç¯
                self._notify_step("âš ï¸ APIé…é¢è€—å°½ï¼Œå¼ºåˆ¶ç»ˆæ­¢æœç´¢å¾ªç¯")
                reflection_result = {
                    "is_sufficient": True,  # å¼ºåˆ¶ç»ˆæ­¢
                    "knowledge_gap": "APIé…é¢è€—å°½ï¼Œæ— æ³•ç»§ç»­åˆ†æ",
                    "follow_up_queries": []
                }
            else:
                # å…¶ä»–é”™è¯¯çš„é™çº§å¤„ç†
                total_content = sum(len(content) for content in all_research_results)
                effort_level = kwargs.get("effort_level", "medium")
                
                # é”™è¯¯æƒ…å†µä¸‹ä½¿ç”¨æ›´å®½æ¾çš„é˜ˆå€¼
                if effort_level == "low":
                    content_threshold = 600 if current_round >= 2 else 1000
                elif effort_level == "medium":
                    content_threshold = 1000 if current_round >= 2 else 1500
                else:  # high
                    content_threshold = 1500 if current_round >= 3 else 2000
                
                reflection_result = {
                    "is_sufficient": total_content > content_threshold or current_round >= total_rounds,
                    "knowledge_gap": "åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ¤æ–­",
                    "follow_up_queries": [] if total_content > content_threshold else [f"{user_query} è¡¥å……ä¿¡æ¯"]
                }
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        is_sufficient = reflection_result.get("is_sufficient", False)
        knowledge_gap = reflection_result.get("knowledge_gap", "")
        
        if is_sufficient:
            self._notify_step(f"âœ… ç¬¬ {current_round} è½®åˆ†æå®Œæˆï¼šä¿¡æ¯å……è¶³ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        else:
            self._notify_step(f"âš ï¸ ç¬¬ {current_round} è½®åˆ†æå®Œæˆï¼š{knowledge_gap}")
            if current_round < total_rounds:
                self._notify_step(f"ğŸ”„ å°†è¿›è¡Œç¬¬ {current_round + 1} è½®è¡¥å……æœç´¢")
        
        # ä¿å­˜åæ€åˆ†æç»“æœ
        reflection_with_metadata = {
            **reflection_result,
            "search_round": current_round,
            "total_research_content": len(all_research_results),
            "content_analysis": f"å·²æ”¶é›† {len(all_research_results)} ä¸ªæœç´¢ç»“æœ"
        }
        self.state_manager.add_reflection_result(reflection_with_metadata)
        
        # å¦‚æœæ˜¯APIé…é¢è€—å°½å¯¼è‡´çš„å¼ºåˆ¶ç»ˆæ­¢ï¼Œæ ‡è®°APIé”™è¯¯
        api_error = False
        if "é…é¢è€—å°½" in knowledge_gap:
            api_error = True
        
        # è®°å½•åˆ†æç»“æœ
        is_sufficient = reflection_result.get("is_sufficient", False)
        total_content_length = sum(len(content) for content in all_research_results)
        
        self._notify_step(f"ğŸ“Š åˆ†æç»“æœ: {'ä¿¡æ¯å……è¶³' if is_sufficient else 'éœ€è¦è¡¥å……æœç´¢'}")
        self._notify_step(f"ğŸ“ˆ å½“å‰ä¿¡æ¯é‡: {total_content_length} å­—ç¬¦, è½®æ¬¡: {current_round}/{total_rounds}")
        
        if not is_sufficient and current_round < total_rounds:
            follow_up_queries = reflection_result.get("follow_up_queries", [])
            self._notify_step(f"ğŸ” è®¡åˆ’è¡¥å……æœç´¢: {len(follow_up_queries)} ä¸ªæŸ¥è¯¢")
        
        return {
            "analysis": reflection_result,
            "is_sufficient": is_sufficient,  # è¿™ä¸ªå­—æ®µå¾ˆé‡è¦ï¼
            "follow_up_queries": reflection_result.get("follow_up_queries", []),
            "current_round": current_round,
            "total_rounds": total_rounds,
            "api_error": api_error
        }
    
    async def _supplementary_search_step(self, **kwargs) -> Dict[str, Any]:
        """è¡¥å……æœç´¢æ­¥éª¤ - åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæ–°çš„æœç´¢æŸ¥è¯¢"""
        analysis = kwargs.get("analysis", {})
        user_query = kwargs.get("user_query", "")
        search_round = kwargs.get("search_round", 0)
        current_round = kwargs.get("current_round", search_round + 1)
        total_rounds = kwargs.get("total_rounds", kwargs.get("max_search_rounds", 3))
        queries_per_round = kwargs.get("queries_per_round", 3)
        
        # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
        if current_round > total_rounds:
            self._notify_step(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æœç´¢è½®æ•°({total_rounds})ï¼Œå¼ºåˆ¶åœæ­¢")
            return {"additional_results": [], "continue_search": False}
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢
        if analysis.get("is_sufficient", True):
            self._notify_step("âœ… ä¿¡æ¯å·²å……è¶³ï¼Œè·³è¿‡è¡¥å……æœç´¢")
            return {"additional_results": [], "continue_search": False}
        
        self._notify_step(f"ğŸ” ç¬¬ {current_round}/{total_rounds} è½®è¡¥å……æœç´¢ä¸­...")
        
        # è·å–ä¹‹å‰è½®æ¬¡çš„æœç´¢ç»“æœä½œä¸ºä¸Šä¸‹æ–‡
        previous_results = self.state_manager.get_successful_search_results()
        previous_context = ""
        if previous_results:
            context_summaries = []
            for result in previous_results[-5:]:  # åªå–æœ€è¿‘5ä¸ªç»“æœä½œä¸ºä¸Šä¸‹æ–‡
                if result.content:
                    context_summaries.append(f"å·²æœç´¢: {result.query}\nç»“æœæ‘˜è¦: {result.content[:200]}...")
            previous_context = "\n\n".join(context_summaries)
        
        # åŸºäºä¸Šä¸‹æ–‡å’Œåˆ†æç»“æœç”Ÿæˆæ–°çš„æœç´¢æŸ¥è¯¢
        follow_up_queries = analysis.get("follow_up_queries", [])
        if not follow_up_queries and previous_context:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„follow_up_queriesï¼ŒåŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆ
            self._notify_step("ğŸ¤” åŸºäºå·²æœ‰ä¿¡æ¯ç”Ÿæˆè¡¥å……æœç´¢æŸ¥è¯¢...")
            try:
                from utils.prompts import PromptTemplates
                context_prompt = f"""
åŸºäºç”¨æˆ·é—®é¢˜: {user_query}

å·²æœ‰æœç´¢ç»“æœ:
{previous_context}

åˆ†æç»“æœæ˜¾ç¤ºä¿¡æ¯ä¸è¶³ã€‚è¯·ç”Ÿæˆ{queries_per_round}ä¸ªè¡¥å……æœç´¢æŸ¥è¯¢ï¼Œç”¨äºè·å–ç¼ºå¤±çš„ä¿¡æ¯ã€‚
æŸ¥è¯¢åº”è¯¥:
1. ä¸å·²æœ‰ç»“æœäº’è¡¥ï¼Œé¿å…é‡å¤
2. é’ˆå¯¹å…·ä½“çš„ä¿¡æ¯ç¼ºå£
3. ä½¿ç”¨ä¸åŒçš„å…³é”®è¯å’Œè§’åº¦

è¯·ç›´æ¥è¿”å›æŸ¥è¯¢åˆ—è¡¨ï¼Œæ¯è¡Œä¸€ä¸ªæŸ¥è¯¢ã€‚
"""
                
                if self.search_agent.client:
                    response = self.search_agent.client.models.generate_content(
                        model=self.model_config.get_model_for_task("search"),
                        contents=context_prompt,
                        config={"temperature": 0.7, "max_output_tokens": 500}
                    )
                    generated_queries = [q.strip() for q in response.text.strip().split('\n') if q.strip()]
                    follow_up_queries = generated_queries[:queries_per_round]
                    
            except Exception as e:
                self._notify_step(f"âš ï¸ è‡ªåŠ¨ç”ŸæˆæŸ¥è¯¢å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤æŸ¥è¯¢: {str(e)}")
        
        # å¦‚æœä»ç„¶æ²¡æœ‰æŸ¥è¯¢ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥
        if not follow_up_queries:
            follow_up_queries = [
                f"{user_query} è¯¦ç»†åˆ†æ",
                f"{user_query} æœ€æ–°å‘å±•",
                f"{user_query} ç›¸å…³æ¡ˆä¾‹"
            ][:queries_per_round]
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡
        follow_up_queries = follow_up_queries[:queries_per_round]
        
        base_progress = 60 + ((current_round - 1) * 15)
        self._notify_progress(f"ç¬¬ {current_round} è½®è¡¥å……æœç´¢", base_progress)
        
        additional_results = []
        
        # æ‰§è¡Œæ‰€æœ‰è¡¥å……æŸ¥è¯¢
        for i, query in enumerate(follow_up_queries):
            query_progress = base_progress + (i * (10 // len(follow_up_queries)))
            self._notify_step(f"ğŸ” è¡¥å……æŸ¥è¯¢ {i+1}/{len(follow_up_queries)}: {query[:50]}...")
            self._notify_progress(f"æ‰§è¡Œè¡¥å……æŸ¥è¯¢ {i+1}", query_progress)
            
            try:
                # Debug: è®°å½•è¡¥å……æœç´¢è¯·æ±‚
                supp_request_id = f"supp_search_{current_round}_{i}_{int(time.time() * 1000)}"
                self.debug_logger.log_api_request(
                    request_type="supplementary_search",
                    model=self.model_config.search_model,
                    prompt=f"ç¬¬{current_round}è½®è¡¥å……æœç´¢: {query}",
                    request_id=supp_request_id,
                    context=f"ç¬¬{current_round}è½®è¡¥å……æœç´¢ {i+1}/{len(follow_up_queries)}"
                )
                
                result = await self.search_agent.search_with_grounding(query)
                
                # Debug: è®°å½•è¡¥å……æœç´¢ç»“æœ
                self.debug_logger.log_search_result(query, result, "supplementary")
                
                # Debug: è®°å½•è¡¥å……æœç´¢å“åº”
                if result.get("success"):
                    response_summary = f"è¡¥å……æœç´¢æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(result.get('content', ''))}, å¼•ç”¨æ•°: {len(result.get('citations', []))}"
                else:
                    response_summary = f"è¡¥å……æœç´¢å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}"
                
                self.debug_logger.log_api_response(
                    request_id=supp_request_id,
                    response_text=response_summary,
                    metadata={
                        "round": current_round,
                        "query_index": i,
                        "success": result.get("success", False),
                        "content_length": len(result.get("content", "")),
                        "citations_count": len(result.get("citations", []))
                    },
                    error=None if result.get("success") else result.get("error", "è¡¥å……æœç´¢å¤±è´¥")
                )
                
                if result.get("success"):
                    self.state_manager.add_search_result(query, result)
                    
                    # ä¿å­˜åˆ°åˆ†æè¿‡ç¨‹ï¼Œæ ‡æ˜è½®æ¬¡å’Œä¸Šä¸‹æ–‡å…³è”
                    web_research_content = f"ç¬¬{current_round}è½®è¡¥å……æŸ¥è¯¢: {query}\nå†…å®¹: {result.get('content', '')}"
                    if result.get('citations'):
                        citations_list = result.get('citations', []) or []
                        citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" 
                                                   for cite in citations_list[:3]])
                        web_research_content += f"\nå¼•ç”¨:\n{citations_text}"
                    self.state_manager.add_web_research_result(web_research_content)
                    
                    additional_results.append(result)
                    self._notify_step(f"âœ… è¡¥å……æŸ¥è¯¢ {i+1} å®Œæˆ")
                    
            except Exception as e:
                error_str = str(e).lower()
                is_quota_exhausted = any(keyword in error_str for keyword in [
                    "quota", "resource_exhausted", "rate limit", "429"
                ])
                
                if is_quota_exhausted:
                    self._notify_step(f"âš ï¸ è¡¥å……æœç´¢APIé…é¢è€—å°½ï¼Œåœæ­¢å½“å‰æœç´¢")
                    return {
                        "additional_results": additional_results,
                        "continue_search": False,
                        "current_round": current_round,
                        "total_rounds": total_rounds,
                        "api_error": True
                    }
                else:
                    self._notify_step(f"âŒ è¡¥å……æœç´¢å¤±è´¥: {str(e)}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
            time.sleep(1)
        
        self._notify_step(f"ğŸ¯ ç¬¬ {current_round} è½®è¡¥å……æœç´¢å®Œæˆï¼Œå…±è·å¾— {len(additional_results)} ä¸ªç»“æœ")
        
        return {
            "additional_results": additional_results,
            "continue_search": True,
            "current_round": current_round,
            "total_rounds": total_rounds
        }
    
    async def _generate_final_answer_step(self, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆæ­¥éª¤"""
        user_query = kwargs.get("user_query", "")
        
        self._notify_step("æ­£åœ¨ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
        self._notify_progress("ç»¼åˆä¿¡æ¯ç”Ÿæˆç­”æ¡ˆ", 90)
        
        # è·å–æ‰€æœ‰æœç´¢ç»“æœ
        all_results = self.state_manager.get_successful_search_results()
        
        if not all_results:
            return {"final_answer": f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äºã€{user_query}ã€çš„ç›¸å…³ä¿¡æ¯ã€‚"}
        
        # å‡†å¤‡æœç´¢ç»“æœå†…å®¹ç”¨äºAIåˆæˆ
        search_summaries = []
        for result in all_results:
            if result.content:
                summary = f"Query: {result.query}\nContent: {result.content}"
                if result.citations:
                    citations_list = result.citations or []
                    citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" for cite in citations_list[:3]])
                    summary += f"\nCitations:\n{citations_text}"
                search_summaries.append(summary)
        
        if not search_summaries:
            return {"final_answer": f"æŠ±æ­‰ï¼Œæœç´¢ç»“æœæ²¡æœ‰åŒ…å«æœ‰æ•ˆå†…å®¹æ¥å›ç­”ã€{user_query}ã€ã€‚"}
        
        # ä½¿ç”¨AIæ¥åˆæˆæœ€ç»ˆç­”æ¡ˆï¼Œè®©AIåˆ¤æ–­ç”¨æˆ·è¯­è¨€
        from utils.prompts import PromptTemplates
        
        synthesis_prompt = PromptTemplates.answer_synthesis_prompt(user_query, search_summaries)
        
        # åœ¨è°ƒç”¨æ¨¡å‹ä¹‹å‰å†æ¬¡é€šçŸ¥ï¼Œè®©ç”¨æˆ·çŸ¥é“æ­£åœ¨è¿›è¡Œè€—æ—¶æ“ä½œ
        self._notify_step(f"è°ƒç”¨æœ€ç»ˆæ¨¡å‹({self.model_config.answer_model})ç”ŸæˆæŠ¥å‘Šï¼Œè¯·è€å¿ƒç­‰å¾…...")
        
        try:
            # ä½¿ç”¨SearchAgentçš„å®¢æˆ·ç«¯æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä½†ä½¿ç”¨answer_model
            if self.search_agent.client:
                self._notify_step("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆè¯¦ç»†ç­”æ¡ˆ...")
                self._notify_progress("AIæ­£åœ¨ç”Ÿæˆç­”æ¡ˆï¼Œè¯·è€å¿ƒç­‰å¾…...", 92)
                
                answer_model = self.model_config.get_model_for_task("answer")
                max_tokens = self.model_config.get_token_limits("answer")
                
                self._notify_step(f"ä½¿ç”¨æ¨¡å‹: {answer_model}, Tokené™åˆ¶: {max_tokens}")
                
                # Debug: è®°å½•æœ€ç»ˆç­”æ¡ˆç”ŸæˆAPIè¯·æ±‚
                answer_request_id = f"final_answer_{int(time.time() * 1000)}"
                self.debug_logger.log_api_request(
                    request_type="final_answer_generation",
                    model=answer_model,
                    prompt=synthesis_prompt,
                    request_id=answer_request_id,
                    context="ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"
                )
                
                response = self.search_agent.client.models.generate_content(
                    model=answer_model,
                    contents=synthesis_prompt,
                    config={
                        "temperature": 0.3,
                        "max_output_tokens": max_tokens
                    }
                )
                
                self._notify_step("AIæ¨¡å‹å“åº”å®Œæˆï¼Œæ­£åœ¨å¤„ç†ç»“æœ...")
                self._notify_progress("ç­”æ¡ˆç”Ÿæˆå®Œæˆ", 95)
                
                final_answer = response.text
                
                # Debug: è®°å½•æœ€ç»ˆç­”æ¡ˆç”ŸæˆAPIå“åº”
                self.debug_logger.log_api_response(
                    request_id=answer_request_id,
                    response_text=final_answer
                )
            else:
                # é™çº§å¤„ç†ï¼šç®€å•æ‹¼æ¥
                combined_content = "\n\n---\n\n".join(search_summaries)
                final_answer = f"""# Research Results for: {user_query}

{combined_content}

Note: This information is gathered from web searches. Please verify for accuracy."""
        
        except Exception as e:
            self._notify_step(f"AIåˆæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼: {str(e)}")
            # é™çº§å¤„ç†ï¼šç®€å•æ‹¼æ¥ä½†æ ¼å¼åŒ–æ›´å¥½
            combined_content = "\n\n---\n\n".join(search_summaries)
            final_answer = f"""# Research Results for: {user_query}

Based on web search results:

{combined_content}

Note: This information is gathered from web searches. Please verify for accuracy."""
        
        return {"final_answer": final_answer}
    
    async def _simple_search_step(self, **kwargs) -> Dict[str, Any]:
        """ç®€å•æœç´¢æ­¥éª¤"""
        user_query = kwargs.get("user_query", "")
        
        self._notify_step("æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...")
        self._notify_progress("æ‰§è¡Œæœç´¢", 50)
        
        result = await self.search_agent.search_with_grounding(user_query)
        
        if result.get("success"):
            self.state_manager.add_search_result(user_query, result)
        
        return {"search_result": result}
    
    async def _generate_simple_answer_step(self, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆç®€å•ç­”æ¡ˆæ­¥éª¤"""
        user_query = kwargs.get("user_query", "")
        search_result = kwargs.get("search_result", {})
        
        self._notify_step("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        self._notify_progress("ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ", 80)
        
        content = search_result.get("content", "")
        citations = search_result.get("citations", [])
        
        if content:
            # å‡†å¤‡æœç´¢ç»“æœç”¨äºAIåˆæˆ
            search_summaries = [f"Query: {user_query}\nContent: {content}"]
            if citations:
                citations_list = citations or []
                citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" for cite in citations_list[:3]])
                search_summaries[0] += f"\nCitations:\n{citations_text}"
            
            # ä½¿ç”¨AIæ¥åˆæˆç­”æ¡ˆï¼Œè®©AIåˆ¤æ–­ç”¨æˆ·è¯­è¨€
            from utils.prompts import PromptTemplates
            synthesis_prompt = PromptTemplates.answer_synthesis_prompt(user_query, search_summaries)
            
            try:
                # ä½¿ç”¨SearchAgentçš„å®¢æˆ·ç«¯æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä½†ä½¿ç”¨answer_model
                if self.search_agent.client:
                    answer_model = self.model_config.get_model_for_task("answer")
                    max_tokens = self.model_config.get_token_limits("answer")
                    
                    response = self.search_agent.client.models.generate_content(
                        model=answer_model,
                        contents=synthesis_prompt,
                        config={
                            "temperature": 0.3,
                            "max_output_tokens": max_tokens
                        }
                    )
                    answer = response.text
                else:
                    # é™çº§å¤„ç†
                    answer = f"""# Answer to: {user_query}

{content}

{f"Sources: {len(citations)} citations" if citations else ""}"""
            
            except Exception as e:
                self._notify_step(f"AIåˆæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼: {str(e)}")
                # é™çº§å¤„ç†
                answer = f"""# Answer to: {user_query}

{content}

{f"Sources: {len(citations)} citations" if citations else ""}"""
        else:
            answer = f"Sorry, no relevant information was found for '{user_query}'. Please try rephrasing your question."
        
        return {"final_answer": answer}
    
    def _notify_progress(self, message: str, percentage: float):
        """é€šçŸ¥è¿›åº¦æ›´æ–°"""
        if self.progress_callback:
            self.progress_callback(message, percentage)
    
    def _notify_step(self, message: str):
        """é€šçŸ¥æ­¥éª¤æ›´æ–°"""
        if self.step_callback:
            self.step_callback(message)
    
    def get_current_task_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰ä»»åŠ¡ä¿¡æ¯"""
        return self.state_manager.get_task_summary()
    
    def export_results(self) -> Dict[str, Any]:
        """å¯¼å‡ºç ”ç©¶ç»“æœ"""
        return self.state_manager.export_session_data()
    
    def clear_session(self):
        """æ¸…é™¤ä¼šè¯æ•°æ®"""
        self.state_manager.clear_session()
        self.search_agent.clear_history()
        
    async def close_clients(self):
        """å…³é—­æ‰€æœ‰å®¢æˆ·ç«¯è¿æ¥"""
        try:
            if hasattr(self.search_agent.client, 'close'):
                await self.search_agent.client.close()
        except Exception as e:
            print(f"å…³é—­å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
    
    @classmethod
    def create_with_config(cls, api_key: str, **config) -> "ResearchEngine":
        """ä½¿ç”¨é…ç½®åˆ›å»ºç ”ç©¶å¼•æ“å®ä¾‹"""
        model_name = config.get("model_name", "gemini-2.0-flash")
        engine = cls(api_key, model_name)
        
        # è®¾ç½®å…¶ä»–é…ç½®
        if "max_search_rounds" in config:
            engine._max_search_rounds = config["max_search_rounds"]
        if "effort_level" in config:
            engine._effort_level = config["effort_level"]
        
        return engine 