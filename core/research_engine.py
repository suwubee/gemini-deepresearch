"""
ç ”ç©¶å¼•æ“æ ¸å¿ƒ
æ•´åˆå·¥ä½œæµæ„å»ºã€æœç´¢ä»£ç†ã€çŠ¶æ€ç®¡ç†ï¼Œå®ç°å®Œæ•´çš„æ·±åº¦ç ”ç©¶åŠŸèƒ½
"""

import asyncio
import time
import traceback
from typing import Dict, List, Any, Optional, Callable
from datetime import datetime

from .workflow_builder import DynamicWorkflowBuilder, DynamicWorkflow, WorkflowStep
from .search_agent import SearchAgent
from .state_manager import StateManager, TaskStatus
from .model_config import ModelConfiguration, get_model_config, set_user_model
from utils.prompts import PromptTemplates
from utils.helpers import extract_json_from_text
from utils.debug_logger import get_debug_logger


class ResearchEngine:
    """æ·±åº¦ç ”ç©¶å¼•æ“æ ¸å¿ƒ"""
    
    def __init__(self, api_key: str, model_name: str = "gemini-2.0-flash"):
        self.api_key = api_key
        
        # è®¾ç½®ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹ï¼Œä½†æœç´¢åŠŸèƒ½å°†å›ºå®šä½¿ç”¨gemini-2.0-flash
        set_user_model(model_name)
        self.model_config = get_model_config()
        
        print(f"ğŸ¤– æ¨¡å‹é…ç½®:")
        print(f"  æœç´¢æ¨¡å‹: {self.model_config.search_model} (å›ºå®š)")
        print(f"  ä»»åŠ¡åˆ†ææ¨¡å‹: {self.model_config.task_analysis_model}")
        print(f"  åæ€æ¨¡å‹: {self.model_config.reflection_model}")
        print(f"  ç­”æ¡ˆç”Ÿæˆæ¨¡å‹: {self.model_config.answer_model}")
        
        # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼Œä½¿ç”¨å¯¹åº”çš„æ¨¡å‹
        self.workflow_builder = DynamicWorkflowBuilder(api_key, self.model_config.task_analysis_model)
        self.search_agent = SearchAgent(api_key, self.model_config.search_model)
        self.state_manager = StateManager()
        self.debug_logger = get_debug_logger()
        
        # è¿›åº¦å›è°ƒå‡½æ•°
        self.progress_callback: Optional[Callable] = None
        self.step_callback: Optional[Callable] = None
        self.error_callback: Optional[Callable] = None
        
        # åœæ­¢æ§åˆ¶æ ‡è®°
        self._stop_research = False
    
    def set_callbacks(self, progress_callback=None, step_callback=None, error_callback=None):
        """è®¾ç½®å›è°ƒå‡½æ•°"""
        if progress_callback:
            self.progress_callback = progress_callback
        if step_callback:
            self.step_callback = step_callback
        if error_callback:
            self.error_callback = error_callback
    
    def set_progress_callback(self, callback: Callable):
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self.progress_callback = callback
    
    def set_step_callback(self, callback: Callable):
        """è®¾ç½®æ­¥éª¤å›è°ƒå‡½æ•°"""
        self.step_callback = callback
    
    def set_error_callback(self, callback: Callable):
        """è®¾ç½®é”™è¯¯å›è°ƒå‡½æ•°"""
        self.error_callback = callback
    
    def stop_research(self):
        """åœæ­¢å½“å‰ç ”ç©¶"""
        self._stop_research = True
        self._notify_step("ğŸ›‘ æ”¶åˆ°åœæ­¢æŒ‡ä»¤ï¼Œæ­£åœ¨ç»ˆæ­¢ç ”ç©¶...")
    
    def reset_stop_flag(self):
        """é‡ç½®åœæ­¢æ ‡è®°"""
        self._stop_research = False
    
    async def research(self, user_query: str, 
                      max_search_rounds: int = 3,
                      effort_level: str = "medium") -> Dict[str, Any]:
        """
        æ‰§è¡Œæ·±åº¦ç ”ç©¶
        
        Args:
            user_query: ç”¨æˆ·æŸ¥è¯¢
            max_search_rounds: æœ€å¤§æœç´¢è½®æ•°
            effort_level: åŠªåŠ›çº§åˆ« (low, medium, high)
            
        Returns:
            ç ”ç©¶ç»“æœå­—å…¸
        """
        try:
            # é‡ç½®åœæ­¢æ ‡è®°
            self._stop_research = False
            
            # Debug: è®°å½•ç ”ç©¶å¼€å§‹
            self.debug_logger.log_workflow_step(
                step_name="research_start",
                step_status="running",
                input_data={
                    "user_query": user_query,
                    "max_search_rounds": max_search_rounds,
                    "effort_level": effort_level
                }
            )
            
            # 1. å¼€å§‹æ–°ä»»åŠ¡
            task_id = self.state_manager.start_new_task(user_query)
            self._notify_progress("å¼€å§‹åˆ†æä»»åŠ¡...", 0)
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if self._stop_research:
                return {"success": False, "error": "ç ”ç©¶è¢«ç”¨æˆ·åœæ­¢"}
            
            # 2. åˆ†æä»»åŠ¡å¹¶æ„å»ºå·¥ä½œæµ
            workflow = await self._analyze_and_build_workflow(user_query, effort_level)
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if self._stop_research:
                return {"success": False, "error": "ç ”ç©¶è¢«ç”¨æˆ·åœæ­¢"}
            
            # 3. æ›¿æ¢å·¥ä½œæµæ­¥éª¤å‡½æ•°ä¸ºå®é™…å®ç°
            self._inject_research_functions(workflow)
            
            # 4. æ‰§è¡Œå·¥ä½œæµ
            self.state_manager.update_task_progress(status=TaskStatus.ANALYZING)
            result = await self._execute_workflow(workflow, user_query, max_search_rounds)
            
            # æ£€æŸ¥åœæ­¢ä¿¡å·
            if self._stop_research:
                return {"success": False, "error": "ç ”ç©¶è¢«ç”¨æˆ·åœæ­¢"}
            
            # 5. å®Œæˆä»»åŠ¡
            self.state_manager.complete_task(result)
            self._notify_progress("ç ”ç©¶å®Œæˆï¼", 100)
            
            final_result = {
                "success": True,
                "task_id": task_id,
                "user_query": user_query,
                "final_answer": result.get("final_answer", ""),
                "search_results": self.state_manager.get_successful_search_results(),
                "citations": self.state_manager.get_all_citations(),
                "urls": self.state_manager.get_unique_urls(),
                "workflow_analysis": self.state_manager.workflow_analysis,
                "task_summary": self.state_manager.get_task_summary(),
                "statistics": self.state_manager.get_session_statistics()
            }
            
            # Debug: è®°å½•ç ”ç©¶å®Œæˆ
            self.debug_logger.log_research_result(
                user_query=user_query,
                final_result=final_result,
                metadata={
                    "task_id": task_id,
                    "effort_level": effort_level,
                    "max_search_rounds": max_search_rounds
                }
            )
            
            self.debug_logger.log_workflow_step(
                step_name="research_complete",
                step_status="completed",
                output_data=final_result
            )
            
            return final_result
            
        except Exception as e:
            # å¤„ç†é”™è¯¯
            error_msg = f"ç ”ç©¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"
            self.state_manager.fail_task(error_msg)
            
            # Debug: è®°å½•é”™è¯¯
            self.debug_logger.log_error(
                error_type="ResearchError",
                error_message=error_msg,
                context={
                    "user_query": user_query,
                    "max_search_rounds": max_search_rounds,
                    "effort_level": effort_level
                },
                stacktrace=traceback.format_exc()
            )
            
            self.debug_logger.log_workflow_step(
                step_name="research_failed",
                step_status="failed",
                output_data={"error": error_msg}
            )
            
            if self.error_callback:
                self.error_callback(error_msg)
            
            return {
                "success": False,
                "error": error_msg,
                "task_summary": self.state_manager.get_task_summary()
            }
    
    async def _analyze_and_build_workflow(self, user_query: str, effort_level: str) -> DynamicWorkflow:
        """åˆ†æä»»åŠ¡å¹¶æ„å»ºå·¥ä½œæµ"""
        self._notify_step("æ­£åœ¨åˆ†æä»»åŠ¡ç±»å‹...")
        self._notify_progress("å¼€å§‹ä»»åŠ¡åˆ†æ", 10)
        
        try:
            workflow = await self.workflow_builder.analyze_task_and_build_workflow(user_query)
            
            # ä¿å­˜å·¥ä½œæµåˆ†æåˆ°çŠ¶æ€ç®¡ç†å™¨
            self.state_manager.set_workflow_analysis(workflow.config)
            self._notify_progress("ä»»åŠ¡åˆ†æå®Œæˆ", 20)
            
            # æ ¹æ®åŠªåŠ›çº§åˆ«è°ƒæ•´å‚æ•°
            self._adjust_workflow_by_effort(workflow, effort_level)
            
            task_type = workflow.config.get("task_type", "Q&A System")
            complexity = workflow.config.get("complexity", "Medium")
            estimated_steps = len(workflow.steps_config)  # ä½¿ç”¨æ­¥éª¤é…ç½®æ•°é‡
            
            print(f"ğŸ” å·¥ä½œæµè¯¦æƒ…: ç±»å‹={task_type}, å¤æ‚åº¦={complexity}, å®é™…æ­¥éª¤={estimated_steps}")
            print(f"ğŸ” æ­¥éª¤åˆ—è¡¨: {[step['name'] for step in workflow.steps_config]}")
            
            self._notify_step(f"ä»»åŠ¡ç±»å‹: {task_type} (å¤æ‚åº¦: {complexity})")
            self._notify_progress(f"å·¥ä½œæµæ„å»ºå®Œæˆï¼Œé¢„è®¡{estimated_steps}æ­¥", 30)
            
            return workflow
            
        except Exception as e:
            error_msg = f"å·¥ä½œæµæ„å»ºå¤±è´¥: {str(e)}"
            self._notify_step(error_msg)
            print(f"âŒ å·¥ä½œæµæ„å»ºå¼‚å¸¸: {e}")
            
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„é™çº§å·¥ä½œæµ
            return self._create_fallback_workflow()
    
    def _create_fallback_workflow(self) -> DynamicWorkflow:
        """åˆ›å»ºé™çº§å·¥ä½œæµ"""
        workflow_config = {
            "task_type": "é—®ç­”ç³»ç»Ÿ",
            "complexity": "ç®€å•",
            "requires_search": True,
            "requires_multiple_rounds": False,
            "estimated_steps": 2,
            "estimated_time": "1-2åˆ†é’Ÿ",
            "reasoning": "é™çº§å·¥ä½œæµï¼šç›´æ¥æœç´¢å’Œå›ç­”"
        }
        steps_config = [
            {"name": "simple_search", "description": "æ‰§è¡Œç®€å•çš„ç½‘ç»œæœç´¢"},
            {"name": "generate_final_answer", "description": "ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"}
        ]
        return DynamicWorkflow(workflow_config, steps_config)
    
    def _adjust_workflow_by_effort(self, workflow: DynamicWorkflow, effort_level: str):
        """æ ¹æ®åŠªåŠ›çº§åˆ«è°ƒæ•´å·¥ä½œæµå‚æ•°ï¼ˆå‚è€ƒåŸå§‹frontendè§„æ ¼ï¼‰"""
        # å‚è€ƒåŸå§‹frontend: 
        # Low: initial_search_query_count=1, max_research_loops=1
        # Medium: initial_search_query_count=3, max_research_loops=3  
        # High: initial_search_query_count=5, max_research_loops=10
        
        # å¼ºåˆ¶è¦†ç›–AIåˆ†æçš„å¤æ‚åº¦ï¼Œä»¥ç”¨æˆ·é€‰æ‹©çš„effortä¸ºå‡†
        if effort_level == "low":
            workflow.config["complexity"] = "Low"
            workflow.config["estimated_steps"] = 3
            workflow.config["estimated_time"] = "1-3åˆ†é’Ÿ"
            workflow.config["max_search_rounds"] = 2  # ä½å¼ºåº¦æœ€å¤š2è½®æœç´¢ï¼ˆåˆå§‹+1æ¬¡è¡¥å……ï¼‰
            self.state_manager.update_settings(
                max_search_results=5,
                max_iterations=2,
                search_timeout=15
            )
        elif effort_level == "high":
            workflow.config["complexity"] = "High"  
            workflow.config["estimated_steps"] = 7
            workflow.config["estimated_time"] = "5-15åˆ†é’Ÿ"
            workflow.config["max_search_rounds"] = 10  # é«˜å¼ºåº¦æœ€å¤š10è½®æœç´¢
            self.state_manager.update_settings(
                max_search_results=20,
                max_iterations=10,
                search_timeout=60
            )
        else:  # medium
            workflow.config["complexity"] = "Medium"
            workflow.config["estimated_steps"] = 5
            workflow.config["estimated_time"] = "3-8åˆ†é’Ÿ"
            workflow.config["max_search_rounds"] = 3  # ä¸­ç­‰å¼ºåº¦æœ€å¤š3è½®æœç´¢
            self.state_manager.update_settings(
                max_search_results=10,
                max_iterations=3,
                search_timeout=30
            )
        
        print(f"ğŸ¯ ç”¨æˆ·effortçº§åˆ«: {effort_level} â†’ å¤æ‚åº¦: {workflow.config['complexity']}, æœ€å¤§æœç´¢è½®æ•°: {workflow.config['max_search_rounds']}")
    
    def _inject_research_functions(self, workflow: DynamicWorkflow):
        """å°†å®é™…çš„ç ”ç©¶å‡½æ•°æ³¨å…¥åˆ°å·¥ä½œæµæ­¥éª¤ä¸­"""
        
        function_mapping = {
            "generate_search_queries": self._generate_search_queries_step,
            "execute_search": self._execute_search_step,
            "analyze_search_results": self._analyze_search_results_step,
            "supplementary_search": self._supplementary_search_step,
            "generate_final_answer": self._generate_final_answer_step,
            "simple_search": self._simple_search_step,
        }

        # æ›¿æ¢ workflow.steps ä¸ºç»‘å®šäº†å‡½æ•°çš„å®Œæ•´ WorkflowStep å®ä¾‹
        injected_steps = []
        for step_config in workflow.steps_config:
            step_name = step_config["name"]
            if step_name in function_mapping:
                step_function = function_mapping[step_name]
                injected_steps.append(WorkflowStep(
                    name=step_name,
                    description=step_config["description"],
                    function=step_function
                ))
            else:
                raise ValueError(f"æœªæ‰¾åˆ°å·¥ä½œæµæ­¥éª¤ '{step_name}' å¯¹åº”çš„å®ç°å‡½æ•°")
        
        workflow.steps = injected_steps
    
    async def _execute_workflow(self, workflow: DynamicWorkflow, 
                               user_query: str, max_search_rounds: int) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµï¼ŒåŒ…å«å¯èƒ½çš„å¤šè½®ç ”ç©¶"""
        
        # ä½¿ç”¨å·¥ä½œæµé…ç½®ä¸­çš„max_search_roundsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
        effective_max_rounds = workflow.config.get("max_search_rounds", max_search_rounds)
        
        context = {
            "user_query": user_query,
            "max_search_rounds": effective_max_rounds
        }
        
        print(f"ğŸ”„ æ‰§è¡Œå·¥ä½œæµï¼Œæœ€å¤§æœç´¢è½®æ•°: {effective_max_rounds}")

        # æ‰§è¡Œåˆå§‹æ­¥éª¤ï¼Œç›´åˆ°éœ€è¦å¾ªç¯çš„"è¡¥å……æœç´¢"æˆ–"æœ€ç»ˆç­”æ¡ˆ"
        for step in workflow.steps:
            if step.name == "supplementary_search":
                break # ç»“æŸåˆå§‹æ­¥éª¤çš„æ‰§è¡Œï¼Œä½†ä¸è·³è¿‡simple_search
            elif step.name == "generate_final_answer":
                # å¦‚æœæ˜¯æœ€åä¸€æ­¥ï¼Œåœ¨å¾ªç¯å¤–å•ç‹¬å¤„ç†
                break
            
            result = await self._execute_step_with_context(step, context)
            context.update(result)

        # å¦‚æœå®šä¹‰äº†è¡¥å……æœç´¢ï¼Œåˆ™è¿›å…¥å¾ªç¯
        supplementary_search_step = next((s for s in workflow.steps if s.name == "supplementary_search"), None)
        if supplementary_search_step:
            current_round = 1
            while current_round < effective_max_rounds:
                # è®¡ç®—å½“å‰è½®æ¬¡çš„è¿›åº¦
                base_progress = 60  # åˆå§‹æœç´¢å®Œæˆåçš„è¿›åº¦
                round_progress = base_progress + (current_round * 15)  # æ¯è½®å¢åŠ 15%
                
                self._notify_step(f"ğŸ”„ ç¬¬ {current_round+1}/{effective_max_rounds} è½®è¡¥å……ç ”ç©¶å¼€å§‹...")
                self._notify_progress(f"æ‰§è¡Œç¬¬ {current_round+1} è½®è¡¥å……æœç´¢", round_progress)
                
                # å¦‚æœåˆ†æåè®¤ä¸ºä¿¡æ¯å……è¶³ï¼Œåˆ™è·³å‡ºå¾ªç¯
                if context.get("is_sufficient"):
                    self._notify_step("âœ… ä¿¡æ¯å·²å……è¶³ï¼Œè·³è¿‡åç»­è¡¥å……ç ”ç©¶")
                    break

                # æ‰§è¡Œè¡¥å……æœç´¢ï¼Œä¼ é€’è½®æ¬¡ä¿¡æ¯
                context["current_round"] = current_round + 1
                context["total_rounds"] = effective_max_rounds
                result = await self._execute_step_with_context(supplementary_search_step, context)
                context.update(result)
                
                # å†æ¬¡åˆ†æç»“æœ
                analyze_step = next((s for s in workflow.steps if s.name == "analyze_search_results"), None)
                if analyze_step:
                    context["search_round"] = current_round  # ä¼ é€’æœç´¢è½®æ¬¡ç»™åˆ†ææ­¥éª¤
                    analysis_result = await self._execute_step_with_context(analyze_step, context)
                    context.update(analysis_result)

                current_round += 1
            
            # å¦‚æœè¾¾åˆ°æœ€å¤§è½®æ•°ä½†ä¿¡æ¯ä»ä¸å……è¶³
            if current_round >= effective_max_rounds and not context.get("is_sufficient"):
                self._notify_step(f"âš ï¸ å·²è¾¾åˆ°æœ€å¤§æœç´¢è½®æ•°({effective_max_rounds})ï¼Œåœæ­¢è¡¥å……æœç´¢")

        # å¾ªç¯ç»“æŸï¼Œæ‰§è¡Œæœ€ç»ˆçš„ç­”æ¡ˆç”Ÿæˆ
        final_answer_step = next((s for s in workflow.steps if s.name == "generate_final_answer"), None)
        if final_answer_step:
            final_result = await self._execute_step_with_context(final_answer_step, context)
            context.update(final_result)
        else:
            raise ValueError("å·¥ä½œæµä¸­æœªå®šä¹‰ 'generate_final_answer' æ­¥éª¤")

        return context
    
    async def _execute_step_with_context(self, step: WorkflowStep, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå·¥ä½œæµæ­¥éª¤å¹¶è¿”å›ç»“æœ"""
        return await step.execute(context)
    
    async def _generate_search_queries_step(self, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆæœç´¢æŸ¥è¯¢æ­¥éª¤"""
        user_query = kwargs.get("user_query", "")
        num_queries = kwargs.get("num_queries", 3)
        
        self._notify_step("æ­£åœ¨ç”Ÿæˆæœç´¢æŸ¥è¯¢...")
        
        queries = await self.search_agent.generate_search_queries(user_query, num_queries)
        
        self.state_manager.add_search_queries(queries)
        
        return {"search_queries": queries}
    
    async def _execute_search_step(self, **kwargs) -> Dict[str, Any]:
        """æ‰§è¡Œæœç´¢æ­¥éª¤"""
        search_queries = kwargs.get("search_queries", [kwargs.get("user_query", "")])
        
        self._notify_step("æ­£åœ¨æ‰§è¡Œç½‘ç»œæœç´¢...")
        
        search_results = []
        for i, query in enumerate(search_queries):
            self._notify_progress(f"æœç´¢æŸ¥è¯¢ {i+1}/{len(search_queries)}: {query[:30]}...", 
                                 40 + (i * 20 // len(search_queries)))
            
            result = await self.search_agent.search_with_grounding(query)
            
            if result.get("success"):
                # å°†ç»“æœæ·»åŠ åˆ°çŠ¶æ€ç®¡ç†å™¨ï¼ˆä¼šè½¬æ¢ä¸º SearchResult å¯¹è±¡ï¼‰
                self.state_manager.add_search_result(query, result)
                
                # ä¿å­˜åˆ°åˆ†æè¿‡ç¨‹ï¼ˆå‚è€ƒåŸå§‹backendç»“æ„ï¼‰
                web_research_content = f"Query: {query}\nContent: {result.get('content', '')}"
                if result.get('citations'):
                    citations_list = result.get('citations', []) or []
                    citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" 
                                               for cite in citations_list[:3]])
                    web_research_content += f"\nCitations:\n{citations_text}"
                self.state_manager.add_web_research_result(web_research_content)
                
                # åœ¨å·¥ä½œæµæ­¥éª¤é—´ä¼ é€’å­—å…¸æ ¼å¼
                search_results.append(result)
        
        return {"search_results": search_results}
    
    async def _analyze_search_results_step(self, **kwargs) -> Dict[str, Any]:
        """åˆ†ææœç´¢ç»“æœæ­¥éª¤ - å‚è€ƒåŸå§‹backendçš„reflectioné€»è¾‘"""
        user_query = kwargs.get("user_query", "")
        search_results = kwargs.get("search_results", [])
        search_round = kwargs.get("search_round", 0)
        current_round = kwargs.get("current_round", search_round + 1)
        total_rounds = kwargs.get("total_rounds", kwargs.get("max_search_rounds", 3))
        
        self._notify_step(f"ğŸ¤” åˆ†æç¬¬ {current_round} è½®æœç´¢ç»“æœï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦ç»§ç»­...")
        self._notify_progress(f"åˆ†æç¬¬ {current_round} è½®ç»“æœ", 70 + ((current_round - 1) * 10))
        
        # è·å–æ‰€æœ‰å·²æœç´¢çš„å†…å®¹ç”¨äºåæ€åˆ†æ
        all_research_results = self.state_manager.get_search_content_list()
        
        # ä½¿ç”¨AIè¿›è¡Œåæ€åˆ†æï¼ˆå‚è€ƒåŸå§‹backendçš„reflectionå‡½æ•°ï¼‰
        from utils.prompts import PromptTemplates
        
        reflection_prompt = f"""åˆ†æä»¥ä¸‹ç ”ç©¶å†…å®¹ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æœç´¢ï¼š

ç ”ç©¶ä¸»é¢˜: {user_query}
å½“å‰æœç´¢è½®æ•°: {current_round}
æœ€å¤§æœç´¢è½®æ•°: {total_rounds}

å·²æ”¶é›†çš„ç ”ç©¶å†…å®¹:
{chr(10).join(['---' + chr(10) + content for content in all_research_results[-5:]])}

è¯·å›ç­”ï¼š
1. å½“å‰ä¿¡æ¯æ˜¯å¦è¶³å¤Ÿå›ç­”ç”¨æˆ·é—®é¢˜ï¼Ÿ(æ˜¯/å¦)
2. å¦‚æœä¸å¤Ÿï¼Œè¿˜éœ€è¦ä»€ä¹ˆä¿¡æ¯ï¼Ÿ
3. å»ºè®®çš„åç»­æœç´¢æŸ¥è¯¢ï¼ˆå¦‚æœéœ€è¦ï¼‰

è¯·ä»¥JSONæ ¼å¼å›ç­”ï¼š
{{
    "is_sufficient": true/false,
    "knowledge_gap": "è¯´æ˜ä¿¡æ¯ç¼ºå£",
    "follow_up_queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", ...]
}}"""

        try:
            if self.search_agent.client:
                reflection_model = self.model_config.get_model_for_task("reflection")
                max_tokens = self.model_config.get_token_limits("reflection")
                
                response = self.search_agent.client.models.generate_content(
                    model=reflection_model,
                    contents=reflection_prompt,
                    config={
                        "temperature": 0.3,
                        "max_output_tokens": max_tokens
                    }
                )
                
                # è§£æAIåæ€ç»“æœ
                import json
                try:
                    reflection_result = json.loads(response.text.strip().replace('```json', '').replace('```', ''))
                except:
                    # é™çº§å¤„ç†
                    reflection_result = {
                        "is_sufficient": len(all_research_results) >= 2,
                        "knowledge_gap": "éœ€è¦æ›´å¤šè¯¦ç»†ä¿¡æ¯",
                        "follow_up_queries": [f"{user_query} è¯¦ç»†åˆ†æ", f"{user_query} æœ€æ–°å‘å±•"]
                    }
            else:
                # é™çº§å¤„ç†ï¼šç®€å•çš„é•¿åº¦åˆ¤æ–­
                total_content = sum(len(content) for content in all_research_results)
                reflection_result = {
                    "is_sufficient": total_content > 2000 or current_round >= total_rounds,
                    "knowledge_gap": "ä¿¡æ¯å……è¶³" if total_content > 2000 else "éœ€è¦æ›´å¤šè¯¦ç»†ä¿¡æ¯",
                    "follow_up_queries": [] if total_content > 2000 else [f"{user_query} è¯¦ç»†åˆ†æ"]
                }
        
        except Exception as e:
            self._notify_step(f"åæ€åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•é€»è¾‘: {str(e)}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯APIé…é¢è€—å°½é”™è¯¯
            error_str = str(e).lower()
            is_quota_exhausted = any(keyword in error_str for keyword in [
                "quota", "resource_exhausted", "rate limit", "429"
            ])
            
            if is_quota_exhausted:
                # å¦‚æœæ˜¯é…é¢è€—å°½ï¼Œå¼ºåˆ¶æ ‡è®°ä¸ºä¿¡æ¯å……è¶³ï¼Œåœæ­¢å¾ªç¯
                self._notify_step("âš ï¸ APIé…é¢è€—å°½ï¼Œå¼ºåˆ¶ç»ˆæ­¢æœç´¢å¾ªç¯")
                reflection_result = {
                    "is_sufficient": True,  # å¼ºåˆ¶ç»ˆæ­¢
                    "knowledge_gap": "APIé…é¢è€—å°½ï¼Œæ— æ³•ç»§ç»­åˆ†æ",
                    "follow_up_queries": []
                }
            else:
                # å…¶ä»–é”™è¯¯çš„é™çº§å¤„ç†
                total_content = sum(len(content) for content in all_research_results)
                reflection_result = {
                    "is_sufficient": total_content > 1500 or current_round >= total_rounds,
                    "knowledge_gap": "åˆ†æå¤±è´¥ï¼Œä½¿ç”¨ç®€å•åˆ¤æ–­",
                    "follow_up_queries": [] if total_content > 1500 else [f"{user_query} è¡¥å……ä¿¡æ¯"]
                }
        
        # æ˜¾ç¤ºåˆ†æç»“æœ
        is_sufficient = reflection_result.get("is_sufficient", False)
        knowledge_gap = reflection_result.get("knowledge_gap", "")
        
        if is_sufficient:
            self._notify_step(f"âœ… ç¬¬ {current_round} è½®åˆ†æå®Œæˆï¼šä¿¡æ¯å……è¶³ï¼Œå‡†å¤‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ")
        else:
            self._notify_step(f"âš ï¸ ç¬¬ {current_round} è½®åˆ†æå®Œæˆï¼š{knowledge_gap}")
            if current_round < total_rounds:
                self._notify_step(f"ğŸ”„ å°†è¿›è¡Œç¬¬ {current_round + 1} è½®è¡¥å……æœç´¢")
        
        # ä¿å­˜åæ€åˆ†æç»“æœ
        reflection_with_metadata = {
            **reflection_result,
            "search_round": current_round,
            "total_research_content": len(all_research_results),
            "content_analysis": f"å·²æ”¶é›† {len(all_research_results)} ä¸ªæœç´¢ç»“æœ"
        }
        self.state_manager.add_reflection_result(reflection_with_metadata)
        
        # å¦‚æœæ˜¯APIé…é¢è€—å°½å¯¼è‡´çš„å¼ºåˆ¶ç»ˆæ­¢ï¼Œæ ‡è®°APIé”™è¯¯
        api_error = False
        if "é…é¢è€—å°½" in knowledge_gap:
            api_error = True
        
        return {
            "analysis": reflection_result,
            "current_round": current_round,
            "total_rounds": total_rounds,
            "api_error": api_error
        }
    
    async def _supplementary_search_step(self, **kwargs) -> Dict[str, Any]:
        """è¡¥å……æœç´¢æ­¥éª¤ - å‚è€ƒåŸå§‹backendçš„å¤šè½®æœç´¢é€»è¾‘"""
        analysis = kwargs.get("analysis", {})
        user_query = kwargs.get("user_query", "")
        search_round = kwargs.get("search_round", 0)
        current_round = kwargs.get("current_round", search_round + 1)  # è·å–å½“å‰è½®æ¬¡
        total_rounds = kwargs.get("total_rounds", kwargs.get("max_search_rounds", 3))
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æœç´¢ï¼ˆå‚è€ƒåŸå§‹backendçš„evaluate_researché€»è¾‘ï¼‰
        if analysis.get("is_sufficient", True):
            self._notify_step("âœ… ä¿¡æ¯å·²å……è¶³ï¼Œè·³è¿‡è¡¥å……æœç´¢")
            return {"additional_results": [], "continue_search": False}
        
        # è·å–follow_up_queries
        follow_up_queries = analysis.get("follow_up_queries", [])
        if not follow_up_queries:
            follow_up_queries = [f"{user_query} è¯¦ç»†åˆ†æ"]
        
        self._notify_step(f"ğŸ” ç¬¬ {current_round}/{total_rounds} è½®è¡¥å……æœç´¢ä¸­...")
        
        # ä½¿ç”¨ä¼ é€’çš„è¿›åº¦ä¿¡æ¯ï¼Œè€Œä¸æ˜¯é‡æ–°è®¡ç®—
        base_progress = 60 + ((current_round - 1) * 15)
        self._notify_progress(f"ç¬¬ {current_round} è½®è¡¥å……æœç´¢", base_progress)
        
        additional_results = []
        
        # æ‰§è¡Œæ‰€æœ‰follow_up_queriesï¼ˆå‚è€ƒåŸå§‹backendé€»è¾‘ï¼‰
        for i, query in enumerate(follow_up_queries[:2]):  # é™åˆ¶æ¯è½®æœ€å¤š2ä¸ªæŸ¥è¯¢
            query_progress = base_progress + (i * 5)  # æ¯ä¸ªæŸ¥è¯¢å¢åŠ 5%è¿›åº¦
            self._notify_step(f"ğŸ” è¡¥å……æŸ¥è¯¢ {i+1}/{len(follow_up_queries[:2])}: {query[:50]}...")
            self._notify_progress(f"æ‰§è¡Œè¡¥å……æŸ¥è¯¢ {i+1}", query_progress)
            
            try:
                result = await self.search_agent.search_with_grounding(query)
                
                if result.get("success"):
                    self.state_manager.add_search_result(query, result)
                    
                    # ä¿å­˜åˆ°åˆ†æè¿‡ç¨‹ï¼Œæ ‡æ˜è½®æ¬¡
                    web_research_content = f"ç¬¬{current_round}è½®è¡¥å……æŸ¥è¯¢: {query}\nå†…å®¹: {result.get('content', '')}"
                    if result.get('citations'):
                        citations_list = result.get('citations', []) or []
                        citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" 
                                                   for cite in citations_list[:3]])
                        web_research_content += f"\nå¼•ç”¨:\n{citations_text}"
                    self.state_manager.add_web_research_result(web_research_content)
                    
                    additional_results.append(result)
                    self._notify_step(f"âœ… è¡¥å……æŸ¥è¯¢ {i+1} å®Œæˆ")
                    
            except Exception as e:
                error_str = str(e).lower()
                is_quota_exhausted = any(keyword in error_str for keyword in [
                    "quota", "resource_exhausted", "rate limit", "429"
                ])
                
                if is_quota_exhausted:
                    self._notify_step(f"âš ï¸ è¡¥å……æœç´¢APIé…é¢è€—å°½ï¼Œåœæ­¢å½“å‰æœç´¢")
                    return {
                        "additional_results": additional_results,
                        "continue_search": False,
                        "current_round": current_round,
                        "total_rounds": total_rounds,
                        "api_error": True
                    }
                else:
                    self._notify_step(f"âŒ è¡¥å……æœç´¢å¤±è´¥: {str(e)}")
            
            # æ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
            time.sleep(1)
        
        self._notify_step(f"ğŸ¯ ç¬¬ {current_round} è½®è¡¥å……æœç´¢å®Œæˆï¼Œå…±è·å¾— {len(additional_results)} ä¸ªç»“æœ")
        
        return {
            "additional_results": additional_results,
            "continue_search": True,
            "current_round": current_round,
            "total_rounds": total_rounds
        }
    
    async def _generate_final_answer_step(self, **kwargs) -> Dict[str, Any]:
        """
        ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
        è¿™æ˜¯å·¥ä½œæµçš„æœ€åä¸€æ­¥
        """
        context = kwargs.get("context", {})
        self._notify_step("æ­£åœ¨ç»¼åˆæ‰€æœ‰ä¿¡æ¯ï¼Œç”Ÿæˆæœ€ç»ˆç ”ç©¶æŠ¥å‘Š...")
        self._notify_progress("ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ", 95)
        
        user_query = context.get("user_query")
        
        # è·å–æ‰€æœ‰æœç´¢ç»“æœ
        all_results = self.state_manager.get_successful_search_results()
        
        if not all_results:
            return {"final_answer": f"æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°å…³äºã€{user_query}ã€çš„ç›¸å…³ä¿¡æ¯ã€‚"}
        
        # å‡†å¤‡æœç´¢ç»“æœå†…å®¹ç”¨äºAIåˆæˆ
        search_summaries = []
        for result in all_results:
            if result.content:
                summary = f"Query: {result.query}\nContent: {result.content}"
                if result.citations:
                    citations_list = result.citations or []
                    citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" for cite in citations_list[:3]])
                    summary += f"\nCitations:\n{citations_text}"
                search_summaries.append(summary)
        
        if not search_summaries:
            return {"final_answer": f"æŠ±æ­‰ï¼Œæœç´¢ç»“æœæ²¡æœ‰åŒ…å«æœ‰æ•ˆå†…å®¹æ¥å›ç­”ã€{user_query}ã€ã€‚"}
        
        # ä½¿ç”¨AIæ¥åˆæˆæœ€ç»ˆç­”æ¡ˆï¼Œè®©AIåˆ¤æ–­ç”¨æˆ·è¯­è¨€
        from utils.prompts import PromptTemplates
        
        synthesis_prompt = PromptTemplates.answer_synthesis_prompt(user_query, search_summaries)
        
        # åœ¨è°ƒç”¨æ¨¡å‹ä¹‹å‰å†æ¬¡é€šçŸ¥ï¼Œè®©ç”¨æˆ·çŸ¥é“æ­£åœ¨è¿›è¡Œè€—æ—¶æ“ä½œ
        self._notify_step(f"è°ƒç”¨æœ€ç»ˆæ¨¡å‹({self.model_config.answer_model})ç”ŸæˆæŠ¥å‘Šï¼Œè¯·è€å¿ƒç­‰å¾…...")

        try:
            # ä½¿ç”¨SearchAgentçš„å®¢æˆ·ç«¯æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä½†ä½¿ç”¨answer_model
            if self.search_agent.client:
                self._notify_step("æ­£åœ¨è°ƒç”¨AIæ¨¡å‹ç”Ÿæˆè¯¦ç»†ç­”æ¡ˆ...")
                self._notify_progress("AIæ­£åœ¨ç”Ÿæˆç­”æ¡ˆï¼Œè¯·è€å¿ƒç­‰å¾…...", 92)
                
                answer_model = self.model_config.get_model_for_task("answer")
                max_tokens = self.model_config.get_token_limits("answer")
                
                self._notify_step(f"ä½¿ç”¨æ¨¡å‹: {answer_model}, Tokené™åˆ¶: {max_tokens}")
                
                response = self.search_agent.client.models.generate_content(
                    model=answer_model,
                    contents=synthesis_prompt,
                    config={
                        "temperature": 0.3,
                        "max_output_tokens": max_tokens
                    }
                )
                
                self._notify_step("AIæ¨¡å‹å“åº”å®Œæˆï¼Œæ­£åœ¨å¤„ç†ç»“æœ...")
                self._notify_progress("ç­”æ¡ˆç”Ÿæˆå®Œæˆ", 95)
                
                final_answer = response.text
            else:
                # é™çº§å¤„ç†ï¼šç®€å•æ‹¼æ¥
                combined_content = "\n\n---\n\n".join(search_summaries)
                final_answer = f"""# Research Results for: {user_query}

{combined_content}

Note: This information is gathered from web searches. Please verify for accuracy."""
        
        except Exception as e:
            self._notify_step(f"AIåˆæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼: {str(e)}")
            # é™çº§å¤„ç†ï¼šç®€å•æ‹¼æ¥ä½†æ ¼å¼åŒ–æ›´å¥½
            combined_content = "\n\n---\n\n".join(search_summaries)
            final_answer = f"""# Research Results for: {user_query}

Based on web search results:

{combined_content}

Note: This information is gathered from web searches. Please verify for accuracy."""
        
        return {"final_answer": final_answer}
    
    async def _simple_search_step(self, **kwargs) -> Dict[str, Any]:
        """ç®€å•æœç´¢æ­¥éª¤"""
        user_query = kwargs.get("user_query", "")
        
        self._notify_step("æ­£åœ¨æœç´¢ç›¸å…³ä¿¡æ¯...")
        self._notify_progress("æ‰§è¡Œæœç´¢", 50)
        
        result = await self.search_agent.search_with_grounding(user_query)
        
        if result.get("success"):
            self.state_manager.add_search_result(user_query, result)
        
        return {"search_result": result}
    
    async def _generate_simple_answer_step(self, **kwargs) -> Dict[str, Any]:
        """ç”Ÿæˆç®€å•ç­”æ¡ˆæ­¥éª¤"""
        user_query = kwargs.get("user_query", "")
        search_result = kwargs.get("search_result", {})
        
        self._notify_step("æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...")
        self._notify_progress("ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ", 80)
        
        content = search_result.get("content", "")
        citations = search_result.get("citations", [])
        
        if content:
            # å‡†å¤‡æœç´¢ç»“æœç”¨äºAIåˆæˆ
            search_summaries = [f"Query: {user_query}\nContent: {content}"]
            if citations:
                citations_list = citations or []
                citations_text = "\n".join([f"- {cite.get('title', 'Unknown Source')}: {cite.get('url', '#')}" for cite in citations_list[:3]])
                search_summaries[0] += f"\nCitations:\n{citations_text}"
            
            # ä½¿ç”¨AIæ¥åˆæˆç­”æ¡ˆï¼Œè®©AIåˆ¤æ–­ç”¨æˆ·è¯­è¨€
            from utils.prompts import PromptTemplates
            synthesis_prompt = PromptTemplates.answer_synthesis_prompt(user_query, search_summaries)
            
            try:
                # ä½¿ç”¨SearchAgentçš„å®¢æˆ·ç«¯æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä½†ä½¿ç”¨answer_model
                if self.search_agent.client:
                    answer_model = self.model_config.get_model_for_task("answer")
                    max_tokens = self.model_config.get_token_limits("answer")
                    
                    response = self.search_agent.client.models.generate_content(
                        model=answer_model,
                        contents=synthesis_prompt,
                        config={
                            "temperature": 0.3,
                            "max_output_tokens": max_tokens
                        }
                    )
                    answer = response.text
                else:
                    # é™çº§å¤„ç†
                    answer = f"""# Answer to: {user_query}

{content}

{f"Sources: {len(citations)} citations" if citations else ""}"""
            
            except Exception as e:
                self._notify_step(f"AIåˆæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€å•æ ¼å¼: {str(e)}")
                # é™çº§å¤„ç†
                answer = f"""# Answer to: {user_query}

{content}

{f"Sources: {len(citations)} citations" if citations else ""}"""
        else:
            answer = f"Sorry, no relevant information was found for '{user_query}'. Please try rephrasing your question."
        
        return {"final_answer": answer}
    
    def _notify_progress(self, message: str, percentage: float):
        """é€šçŸ¥è¿›åº¦æ›´æ–°"""
        if self.progress_callback:
            self.progress_callback(message, percentage)
    
    def _notify_step(self, message: str):
        """é€šçŸ¥æ­¥éª¤æ›´æ–°"""
        if self.step_callback:
            self.step_callback(message)
    
    def get_current_task_info(self) -> Dict[str, Any]:
        """è·å–å½“å‰ä»»åŠ¡ä¿¡æ¯"""
        return self.state_manager.get_task_summary()
    
    def export_results(self) -> Dict[str, Any]:
        """å¯¼å‡ºç ”ç©¶ç»“æœ"""
        return self.state_manager.export_session_data()
    
    def clear_session(self):
        """æ¸…é™¤ä¼šè¯æ•°æ®"""
        self.state_manager.clear_session()
        self.search_agent.clear_history() 