"""
Streamlit è¾…åŠ©å‡½æ•°
ç”¨äºæ˜¾ç¤ºç ”ç©¶ç»“æœå’Œç•Œé¢ç»„ä»¶
"""

import streamlit as st
import json
from datetime import datetime
from enum import Enum


def json_serializable(obj):
    """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    if hasattr(obj, '__dict__'):
        return {k: json_serializable(v) for k, v in obj.__dict__.items()}
    elif isinstance(obj, Enum):
        return obj.value
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif isinstance(obj, (list, tuple)):
        return [json_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: json_serializable(value) for key, value in obj.items()}
    else:
        return obj


def create_markdown_content(research_results):
    """åˆ›å»ºMarkdownæ ¼å¼çš„ç ”ç©¶æŠ¥å‘Š"""
    final_answer = research_results.get("final_answer", "")
    user_query = research_results.get("user_query", "")
    task_id = research_results.get("task_id", "")
    
    markdown_content = f"# ğŸ” DeepSearch ç ”ç©¶æŠ¥å‘Š\n\n"
    markdown_content += f"**ç ”ç©¶ä¸»é¢˜:** {user_query}\n\n"
    markdown_content += f"**ä»»åŠ¡ID:** {task_id}\n\n"
    markdown_content += "---\n\n"
    
    # æ·»åŠ ä¸»è¦ç ”ç©¶ç»“æœ
    if final_answer:
        markdown_content += "## ğŸ“‹ ç ”ç©¶ç»“æœ\n\n"
        markdown_content += final_answer
        markdown_content += "\n\n"
    
    # æ·»åŠ å¼•ç”¨æ¥æº
    citations = research_results.get("citations", [])
    if citations:
        markdown_content += "## ğŸ“š å¼•ç”¨æ¥æº\n\n"
        for i, citation in enumerate(citations[:10], 1):
            title = citation.get("title", f"æ¥æº {i}")
            url = citation.get("url", "#")
            markdown_content += f"{i}. [{title}]({url})\n"
        markdown_content += "\n"
    
    # æ·»åŠ ç›¸å…³é“¾æ¥
    urls = research_results.get("urls", [])
    if urls:
        markdown_content += "## ğŸ”— ç›¸å…³é“¾æ¥\n\n"
        for url in urls[:10]:
            markdown_content += f"- {url}\n"
        markdown_content += "\n"
    
    # æ·»åŠ æœç´¢ç»Ÿè®¡
    search_results = research_results.get("search_results", [])
    if search_results:
        markdown_content += f"## ğŸ“Š ç ”ç©¶ç»Ÿè®¡\n\n"
        markdown_content += f"- æœç´¢æ¬¡æ•°ï¼š{len(search_results)}\n"
        successful_searches = len([r for r in search_results if r.get('success')])
        markdown_content += f"- æˆåŠŸæœç´¢ï¼š{successful_searches}\n"
        total_citations = sum(len(r.get('citations', [])) for r in search_results)
        markdown_content += f"- æ€»å¼•ç”¨æ•°ï¼š{total_citations}\n\n"
    
    # æ·»åŠ ä»»åŠ¡æ‘˜è¦
    task_summary = research_results.get("task_summary", {})
    if task_summary:
        markdown_content += "## âš™ï¸ ä»»åŠ¡ä¿¡æ¯\n\n"
        if "task_id" in task_summary:
            markdown_content += f"- ä»»åŠ¡IDï¼š{task_summary['task_id']}\n"
        if "duration" in task_summary:
            duration = task_summary["duration"]
            markdown_content += f"- æ‰§è¡Œæ—¶é•¿ï¼š{duration:.1f}ç§’\n"
        if "status" in task_summary:
            status = task_summary["status"]
            if isinstance(status, Enum):
                status = status.value
            markdown_content += f"- æ‰§è¡ŒçŠ¶æ€ï¼š{status}\n"
    
    # æ·»åŠ ç”Ÿæˆæ—¶é—´
    markdown_content += f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*\n"
    markdown_content += "*ç”± ğŸ” DeepSearch æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ç”Ÿæˆ*"
    
    return markdown_content


def display_task_analysis(workflow_analysis, task_id):
    """æ˜¾ç¤ºä»»åŠ¡åˆ†æç»“æœ"""
    if not workflow_analysis:
        return
    
    st.markdown(f"### ğŸ“Š ä»»åŠ¡åˆ†æç»“æœ ({task_id[:20]})")
    col1, col2 = st.columns(2)
    
    with col1:
        st.metric("ä»»åŠ¡ç±»å‹", workflow_analysis.task_type)
        st.metric("å¤æ‚åº¦", workflow_analysis.complexity)
        st.metric("é¢„ä¼°æ­¥éª¤", workflow_analysis.estimated_steps)
    
    with col2:
        st.metric("éœ€è¦æœç´¢", "æ˜¯" if workflow_analysis.requires_search else "å¦")
        st.metric("å¤šè½®æœç´¢", "æ˜¯" if workflow_analysis.requires_multiple_rounds else "å¦")
        st.metric("é¢„ä¼°æ—¶é—´", workflow_analysis.estimated_time)
    
    if workflow_analysis.reasoning:
        st.text_area("åˆ†ææ¨ç†", workflow_analysis.reasoning, height=100, disabled=True, key=f"reasoning_{task_id}")


def display_search_results(research_results):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    if not research_results or not research_results.get("search_results"):
        return
    
    search_results = research_results["search_results"]
    task_id = research_results.get("task_id", "default")
    
    st.markdown(f"### ğŸ” æœç´¢ç»“æœ ({len(search_results)}) ({task_id[:20]})")
    for i, result in enumerate(search_results, 1):
        with st.container():
            # ä½¿ç”¨å­—å…¸è®¿é—®æ–¹å¼è€Œä¸æ˜¯å¯¹è±¡å±æ€§è®¿é—®
            query = result.get("query", "æœªçŸ¥æŸ¥è¯¢") if isinstance(result, dict) else getattr(result, 'query', "æœªçŸ¥æŸ¥è¯¢")
            st.markdown(f"**æœç´¢ {i}: {query}**")
            
            success = result.get("success", False) if isinstance(result, dict) else getattr(result, 'success', False)
            if success:
                duration = result.get("duration", 0) if isinstance(result, dict) else getattr(result, 'duration', 0)
                st.success(f"âœ… æœç´¢æˆåŠŸ (è€—æ—¶: {duration:.2f}ç§’)")
                
                content = result.get("content", "") if isinstance(result, dict) else getattr(result, 'content', "")
                if content:
                    content_preview = content[:200] + "..." if len(content) > 200 else content
                    st.text_area(f"å†…å®¹é¢„è§ˆ", content_preview, height=100, disabled=True, key=f"content_{task_id}_{i}")
                
                citations = result.get("citations", []) if isinstance(result, dict) else getattr(result, 'citations', [])
                if citations:
                    st.markdown("**å¼•ç”¨æ¥æº:**")
                    citations_list = citations or []
                    for j, citation in enumerate(citations_list[:3]):
                        title = citation.get("title", "æœªçŸ¥æ ‡é¢˜")
                        url = citation.get("url", "#")
                        if title and title != "æœªçŸ¥æ ‡é¢˜":
                            st.markdown(f"- [{title}]({url})")
                        else:
                            st.markdown(f"- {url}")
            else:
                error = result.get("error", "æœªçŸ¥é”™è¯¯") if isinstance(result, dict) else getattr(result, 'error', "æœªçŸ¥é”™è¯¯")
                st.error(f"âŒ æœç´¢å¤±è´¥: {error}")
            
            st.divider()


def display_final_answer(research_results, index=None):
    """æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ"""
    final_answer = research_results.get("final_answer", "")
    task_id = research_results.get("task_id", "default")
    
    # åˆ›å»ºå”¯ä¸€çš„keyåç¼€
    key_suffix = f"_{index}" if index is not None else ""
    unique_key = f"{task_id}{key_suffix}"
    
    if final_answer:
        # æ·»åŠ æ ‡é¢˜å’Œæ“ä½œæŒ‰é’®è¡Œ
        col1, col2 = st.columns([5, 1])
        
        with col1:
            st.markdown("### ğŸ¯ ç ”ç©¶ç»“æœ")
        
        # åˆ‡æ¢æ˜¾ç¤ºmarkdownå†…å®¹çš„ä¼šè¯çŠ¶æ€
        if f"show_markdown_{unique_key}" not in st.session_state:
            st.session_state[f"show_markdown_{unique_key}"] = False

        with col2:
            if st.button("ğŸ“‹ å¤åˆ¶æŠ¥å‘Š", help="ç”Ÿæˆå®Œæ•´çš„MarkdownæŠ¥å‘Šä»¥ä¾›å¤åˆ¶", key=f"copy_md_{unique_key}"):
                st.session_state[f"show_markdown_{unique_key}"] = not st.session_state[f"show_markdown_{unique_key}"]

        # æ ¹æ®çŠ¶æ€æ˜¾ç¤ºæˆ–éšè—markdowné¢„è§ˆ
        if st.session_state.get(f"show_markdown_{unique_key}", False):
            try:
                markdown_content = create_markdown_content(research_results)
                st.code(markdown_content, language="markdown")
                st.success("âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·ä»ä¸Šæ–¹å¤åˆ¶ä»£ç å—ä¸­çš„å†…å®¹ã€‚")
            except Exception as e:
                st.error(f"âŒ ç”ŸæˆMarkdownå¤±è´¥: {str(e)}")
        
        # æ˜¾ç¤ºä¸»è¦ç ”ç©¶ç»“æœ
        st.markdown(final_answer)
        
        # ä»StateManagerè·å–å¼•ç”¨å’Œæ¥æº
        if st.session_state.research_engine:
            citations = st.session_state.research_engine.state_manager.get_all_citations()
            urls = st.session_state.research_engine.state_manager.get_unique_urls()
            analysis_process = st.session_state.research_engine.state_manager.get_analysis_process()
        else:
            citations = []
            urls = []
            analysis_process = {}
        
        # æ˜¾ç¤ºåˆ†æè¿‡ç¨‹ï¼ˆå‚è€ƒåŸå§‹backendç»“æ„ï¼‰
        if analysis_process:
            # ä½¿ç”¨å®¹å™¨å’Œtabsæ¥é¿å…åµŒå¥—expanderé—®é¢˜
            st.markdown(f"### ğŸ”¬ åˆ†æè¿‡ç¨‹ ({task_id[:20]})")
            tab1, tab2, tab3, tab4 = st.tabs([
                "æœç´¢æŸ¥è¯¢", 
                "æœç´¢ç»“æœ", 
                "åˆ†æåæ€", 
                "ç»Ÿè®¡ä¿¡æ¯"
            ])
            
            with tab1:
                # æ˜¾ç¤ºæœç´¢æŸ¥è¯¢
                search_queries = analysis_process.get("search_queries", [])
                if search_queries:
                    st.markdown("**æœç´¢æŸ¥è¯¢:**")
                    for i, query in enumerate(search_queries, 1):
                        st.markdown(f"{i}. {query}")
                else:
                    st.info("æš‚æ— æœç´¢æŸ¥è¯¢è®°å½•")
            
            with tab2:
                # æ˜¾ç¤ºç½‘ç»œæœç´¢ç»“æœ
                web_research_results = analysis_process.get("web_research_results", [])
                if web_research_results:
                    st.markdown("**ç½‘ç»œæœç´¢ç»“æœ:**")
                    for i, result in enumerate(web_research_results, 1):
                        st.markdown(f"**æœç´¢ç»“æœ {i}:**")
                        # ä½¿ç”¨ä»£ç å—æ˜¾ç¤ºè€Œä¸æ˜¯åµŒå¥—expander
                        st.code(result, language=None)
                        st.divider()
                else:
                    st.info("æš‚æ— æœç´¢ç»“æœè®°å½•")
            
            with tab3:
                # æ˜¾ç¤ºåæ€åˆ†æ
                reflection_results = analysis_process.get("reflection_results", [])
                if reflection_results:
                    st.markdown("**åˆ†æåæ€:**")
                    for i, reflection in enumerate(reflection_results, 1):
                        st.markdown(f"**åˆ†æ {i}:**")
                        st.json(reflection)
                        st.divider()
                else:
                    st.info("æš‚æ— åˆ†æåæ€è®°å½•")
            
            with tab4:
                # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("æœç´¢ç»“æœæ•°", analysis_process.get("search_results_count", 0))
                with col2:
                    st.metric("æˆåŠŸæœç´¢æ•°", analysis_process.get("successful_searches", 0))
        
        # å¼•ç”¨å’Œæ¥æº
        if citations or urls:
            st.markdown(f"### ğŸ“š å¼•ç”¨å’Œæ¥æº ({task_id[:20]})")
            if citations:
                st.markdown("**å¼•ç”¨æ¥æº:**")
                for i, citation in enumerate(citations, 1):
                    url = citation.get("url", "#")
                    title = citation.get("title", "æœªçŸ¥æ ‡é¢˜")
                    if title and title != "æœªçŸ¥æ ‡é¢˜":
                        st.markdown(f"**{i}.** [{title}]({url})")
                    else:
                        st.markdown(f"**{i}.** {url}")
                st.divider()
            
            if urls:
                st.markdown("**ç›¸å…³é“¾æ¥:**")
                urls_list = urls or []
                for url in urls_list[:10]:
                    st.markdown(f"- {url}")
