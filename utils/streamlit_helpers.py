"""
Streamlit ç›¸å…³çš„è¾…åŠ©å‡½æ•°
"""

import streamlit as st
from datetime import datetime
from enum import Enum

def json_serializable(obj):
    """å°†å¯¹è±¡è½¬æ¢ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
    if isinstance(obj, Enum):
        return obj.value
    elif isinstance(obj, datetime):
        return obj.isoformat()
    elif hasattr(obj, '__dict__'):
        return {k: json_serializable(v) for k, v in obj.__dict__.items()}
    elif isinstance(obj, (list, tuple)):
        return [json_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {k: json_serializable(v) for k, v in obj.items()}
    else:
        return obj


def create_markdown_content(research_results):
    """åˆ›å»ºmarkdownæ ¼å¼çš„ç ”ç©¶å†…å®¹"""
    if not research_results or not research_results.get("success"):
        return "# ç ”ç©¶ç»“æœ\n\næ²¡æœ‰å¯ç”¨çš„ç ”ç©¶ç»“æœã€‚"
    
    user_query = research_results.get("user_query", "æœªçŸ¥æŸ¥è¯¢")
    final_answer = research_results.get("final_answer", "")
    
    # æ„å»ºmarkdownå†…å®¹
    markdown_content = f"""# ğŸ” ç ”ç©¶æŠ¥å‘Š

## ğŸ“‹ ç ”ç©¶ä¸»é¢˜
{user_query}

## ğŸ¯ ç ”ç©¶ç»“æœ
{final_answer}

---

"""
    
    # æ·»åŠ å¼•ç”¨æ¥æº
    citations = research_results.get("citations", [])
    if citations:
        markdown_content += "## ğŸ“š å¼•ç”¨æ¥æº\n\n"
        for i, citation in enumerate(citations[:10], 1):
            title = citation.get("title", f"æ¥æº {i}")
            url = citation.get("url", "#")
            markdown_content += f"{i}. [{title}]({url})\n"
        markdown_content += "\n"
    
    # æ·»åŠ æœç´¢ç»Ÿè®¡
    search_results = research_results.get("search_results", [])
    if search_results:
        markdown_content += f"## ğŸ“Š ç ”ç©¶ç»Ÿè®¡\n\n"
        markdown_content += f"- æœç´¢æ¬¡æ•°ï¼š{len(search_results)}\n"
        successful_searches = len([r for r in search_results if r.get('success')])
        markdown_content += f"- æˆåŠŸæœç´¢ï¼š{successful_searches}\n"
        total_citations = sum(len(r.get('citations', [])) for r in search_results)
        markdown_content += f"- æ€»å¼•ç”¨æ•°ï¼š{total_citations}\n\n"
    
    # æ·»åŠ ä»»åŠ¡æ‘˜è¦
    task_summary = research_results.get("task_summary", {})
    if task_summary:
        markdown_content += "## âš™ï¸ ä»»åŠ¡ä¿¡æ¯\n\n"
        if "task_id" in task_summary:
            markdown_content += f"- ä»»åŠ¡IDï¼š{task_summary['task_id']}\n"
        if "duration" in task_summary:
            duration = task_summary["duration"]
            markdown_content += f"- æ‰§è¡Œæ—¶é•¿ï¼š{duration:.1f}ç§’\n"
        if "status" in task_summary:
            status = task_summary["status"]
            if isinstance(status, Enum):
                status = status.value
            markdown_content += f"- æ‰§è¡ŒçŠ¶æ€ï¼š{status}\n"
    
    # æ·»åŠ ç”Ÿæˆæ—¶é—´
    markdown_content += f"\n---\n\n*æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}*\n"
    markdown_content += "*ç”± ğŸ” DeepSearch æ™ºèƒ½ç ”ç©¶åŠ©æ‰‹ç”Ÿæˆ*"
    
    return markdown_content


def display_task_analysis(workflow_analysis, task_id):
    """æ˜¾ç¤ºä»»åŠ¡åˆ†æç»“æœ"""
    if not workflow_analysis:
        return
    
    with st.expander(f"ğŸ“Š ä»»åŠ¡åˆ†æç»“æœ ({task_id[:8]})", expanded=False):
        col1, col2 = st.columns(2)
        
        with col1:
            st.metric("ä»»åŠ¡ç±»å‹", workflow_analysis.task_type)
            st.metric("å¤æ‚åº¦", workflow_analysis.complexity)
            st.metric("é¢„ä¼°æ­¥éª¤", workflow_analysis.estimated_steps)
        
        with col2:
            st.metric("éœ€è¦æœç´¢", "æ˜¯" if workflow_analysis.requires_search else "å¦")
            st.metric("å¤šè½®æœç´¢", "æ˜¯" if workflow_analysis.requires_multiple_rounds else "å¦")
            st.metric("é¢„ä¼°æ—¶é—´", workflow_analysis.estimated_time)
        
        if workflow_analysis.reasoning:
            st.text_area("åˆ†ææ¨ç†", workflow_analysis.reasoning, height=100, disabled=True, key=f"reasoning_{task_id}")


def display_search_results(research_results):
    """æ˜¾ç¤ºæœç´¢ç»“æœ"""
    if not research_results or not research_results.get("search_results"):
        return
    
    search_results = research_results["search_results"]
    task_id = research_results.get("task_id", "default")
    
    with st.expander(f"ğŸ” æœç´¢ç»“æœ ({len(search_results)}) ({task_id[:8]})", expanded=False):
        for i, result in enumerate(search_results, 1):
            with st.container():
                st.markdown(f"**æœç´¢ {i}: {result.query}**")
                
                if result.success:
                    st.success(f"âœ… æœç´¢æˆåŠŸ (è€—æ—¶: {result.duration:.2f}ç§’)")
                    
                    if result.content:
                        content_preview = result.content[:200] + "..." if len(result.content) > 200 else result.content
                        st.text_area(f"å†…å®¹é¢„è§ˆ", content_preview, height=100, disabled=True, key=f"content_{task_id}_{i}")
                    
                    if result.citations:
                        st.markdown("**å¼•ç”¨æ¥æº:**")
                        citations_list = result.citations or []
                        for j, citation in enumerate(citations_list[:3]):
                            title = citation.get("title", "æœªçŸ¥æ ‡é¢˜")
                            url = citation.get("url", "#")
                            st.markdown(f"- [{title}]({url})")
                else:
                    st.error(f"âŒ æœç´¢å¤±è´¥: {result.error}")
                
                st.divider()


def display_final_answer(research_results):
    """æ˜¾ç¤ºæœ€ç»ˆç­”æ¡ˆ"""
    final_answer = research_results.get("final_answer", "")
    task_id = research_results.get("task_id", "default")
    
    if final_answer:
        # æ·»åŠ æ ‡é¢˜å’Œæ“ä½œæŒ‰é’®è¡Œ
        col1, col2 = st.columns([5, 1])
        
        with col1:
            st.markdown("### ğŸ¯ ç ”ç©¶ç»“æœ")
        
        # åˆ‡æ¢æ˜¾ç¤ºmarkdownå†…å®¹çš„ä¼šè¯çŠ¶æ€
        if f"show_markdown_{task_id}" not in st.session_state:
            st.session_state[f"show_markdown_{task_id}"] = False

        with col2:
            if st.button("ğŸ“‹ å¤åˆ¶æŠ¥å‘Š", help="ç”Ÿæˆå®Œæ•´çš„MarkdownæŠ¥å‘Šä»¥ä¾›å¤åˆ¶", key=f"copy_md_{task_id}"):
                st.session_state[f"show_markdown_{task_id}"] = not st.session_state[f"show_markdown_{task_id}"]

        # æ ¹æ®çŠ¶æ€æ˜¾ç¤ºæˆ–éšè—markdowné¢„è§ˆ
        if st.session_state.get(f"show_markdown_{task_id}", False):
            try:
                markdown_content = create_markdown_content(research_results)
                st.code(markdown_content, language="markdown")
                st.success("âœ… MarkdownæŠ¥å‘Šå·²ç”Ÿæˆï¼Œè¯·ä»ä¸Šæ–¹å¤åˆ¶ä»£ç å—ä¸­çš„å†…å®¹ã€‚")
            except Exception as e:
                st.error(f"âŒ ç”ŸæˆMarkdownå¤±è´¥: {str(e)}")
        
        # æ˜¾ç¤ºä¸»è¦ç ”ç©¶ç»“æœ
        st.markdown(final_answer)
        
        # ä»StateManagerè·å–å¼•ç”¨å’Œæ¥æº
        if st.session_state.research_engine:
            citations = st.session_state.research_engine.state_manager.get_all_citations()
            urls = st.session_state.research_engine.state_manager.get_unique_urls()
            analysis_process = st.session_state.research_engine.state_manager.get_analysis_process()
        else:
            citations = []
            urls = []
            analysis_process = {}
        
        # æ˜¾ç¤ºåˆ†æè¿‡ç¨‹ï¼ˆå‚è€ƒåŸå§‹backendç»“æ„ï¼‰
        if analysis_process:
            with st.expander(f"ğŸ”¬ åˆ†æè¿‡ç¨‹ ({task_id[:8]})", expanded=False):
                # ä½¿ç”¨tabsæ¥é¿å…åµŒå¥—expanderé—®é¢˜
                tab1, tab2, tab3, tab4 = st.tabs([
                    f"æœç´¢æŸ¥è¯¢_{task_id}", 
                    f"æœç´¢ç»“æœ_{task_id}", 
                    f"åˆ†æåæ€_{task_id}", 
                    f"ç»Ÿè®¡ä¿¡æ¯_{task_id}"
                ])
                
                with tab1:
                    # æ˜¾ç¤ºæœç´¢æŸ¥è¯¢
                    search_queries = analysis_process.get("search_queries", [])
                    if search_queries:
                        st.markdown("**æœç´¢æŸ¥è¯¢:**")
                        for i, query in enumerate(search_queries, 1):
                            st.markdown(f"{i}. {query}")
                    else:
                        st.info("æš‚æ— æœç´¢æŸ¥è¯¢è®°å½•")
                
                with tab2:
                    # æ˜¾ç¤ºç½‘ç»œæœç´¢ç»“æœ
                    web_research_results = analysis_process.get("web_research_results", [])
                    if web_research_results:
                        st.markdown("**ç½‘ç»œæœç´¢ç»“æœ:**")
                        for i, result in enumerate(web_research_results, 1):
                            st.markdown(f"**æœç´¢ç»“æœ {i}:**")
                            # ä½¿ç”¨ä»£ç å—æ˜¾ç¤ºè€Œä¸æ˜¯åµŒå¥—expander
                            st.code(result, language=None)
                            st.divider()
                    else:
                        st.info("æš‚æ— æœç´¢ç»“æœè®°å½•")
                
                with tab3:
                    # æ˜¾ç¤ºåæ€åˆ†æ
                    reflection_results = analysis_process.get("reflection_results", [])
                    if reflection_results:
                        st.markdown("**åˆ†æåæ€:**")
                        for i, reflection in enumerate(reflection_results, 1):
                            st.markdown(f"**åˆ†æ {i}:**")
                            st.json(reflection)
                            st.divider()
                    else:
                        st.info("æš‚æ— åˆ†æåæ€è®°å½•")
                
                with tab4:
                    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("æœç´¢ç»“æœæ•°", analysis_process.get("search_results_count", 0))
                    with col2:
                        st.metric("æˆåŠŸæœç´¢æ•°", analysis_process.get("successful_searches", 0))
        
        # å¼•ç”¨å’Œæ¥æº
        if citations or urls:
            with st.expander(f"ğŸ“š å¼•ç”¨å’Œæ¥æº ({task_id[:8]})", expanded=False):
                if citations:
                    st.markdown("**å¼•ç”¨æ¥æº:**")
                    for i, citation in enumerate(citations, 1):
                        title = citation.get("title", f"æ¥æº {i}")
                        url = citation.get("url", "#")
                        # ä¸å†æ˜¾ç¤º"Source from"ï¼Œç›´æ¥æ˜¾ç¤ºç½‘é¡µæ ‡é¢˜
                        
                        # ä¿®å¤ï¼šå¦‚æœURLæ˜¯redirecté“¾æ¥ï¼Œå°è¯•æ˜ å°„åˆ°å®é™…åŸŸå
                        if "vertexaisearch.cloud.google.com" in url:
                            # æ ¹æ®æ ‡é¢˜å°è¯•æ¨å¯¼åŸå§‹URL
                            if title and title != f"æ¥æº {i}":
                                clean_title = title.split('.')[0].lower()
                                domain_map = {
                                    'bondcap': 'bondcap.com',
                                    'zdnet': 'zdnet.com',
                                    'techcrunch': 'techcrunch.com',
                                    'reuters': 'reuters.com',
                                    'bloomberg': 'bloomberg.com',
                                    'cnbc': 'cnbc.com',
                                    'forbes': 'forbes.com',
                                    'marketwatch': 'marketwatch.com'
                                }
                                for key, domain in domain_map.items():
                                    if key in clean_title:
                                        url = f"https://{domain}"
                                        break
                        
                        st.markdown(f"**{i}. [{title}]({url})**")
                        st.divider()
                
                if urls:
                    st.markdown("**ç›¸å…³é“¾æ¥:**")
                    urls_list = urls or []
                    for url in urls_list[:10]:
                        st.markdown(f"- {url}") 